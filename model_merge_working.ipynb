{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 4\n",
      "GPU 0: NVIDIA A100 80GB PCIe\n",
      "GPU 1: NVIDIA A100 80GB PCIe\n",
      "GPU 2: NVIDIA A100 80GB PCIe\n",
      "GPU 3: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mert/miniconda3/envs/spec/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergedModel(torch.nn.Module):\n",
    "    def __init__(self, tokenizer, base_model, reward_model, causal_model, beginning_offset, total_tokens, device):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.base_model = base_model.to(device)\n",
    "        self.reward_model = reward_model\n",
    "        self.causal_model = causal_model\n",
    "        self.lm_head = causal_model.lm_head.to(device)\n",
    "        self.reward_score = reward_model.score.to(device)\n",
    "        self.beginning_offset = beginning_offset\n",
    "        self.total_tokens = total_tokens\n",
    "        self.device = device\n",
    "        \n",
    "    def get_log_probs(self, lm_outputs, input_ids):\n",
    "        probs = []\n",
    "        lm_prob_dist = F.softmax(lm_outputs, dim=-1)\n",
    "        for offset in range(self.beginning_offset, self.beginning_offset+self.total_tokens):\n",
    "            prob = lm_prob_dist[0,-offset-1,input_ids[0][-offset]].item()\n",
    "            probs.append(prob)\n",
    "        return probs\n",
    "    \n",
    "    def get_reward_scores(self, input_ids, base_outputs):\n",
    "        reward_outputs = self.reward_score(base_outputs[0])\n",
    "        step_sep_id = self.tokenizer.encode(\"<extra_0>\")[0]\n",
    "        token_masks = (input_ids == step_sep_id)\n",
    "        logits = reward_outputs[0]\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        probabilities = probabilities * token_masks.unsqueeze(-1) # bs, seq_len, num_labels\n",
    "\n",
    "        all_scores_res = []\n",
    "        for i in range(probabilities.size(0)):\n",
    "            sample = probabilities[i] # seq_len, num_labels\n",
    "            positive_probs = sample[sample != 0].view(-1, 2)[:, 1] # valid_tokens, num_labels\n",
    "            non_zero_elements_list = positive_probs.cpu().tolist()\n",
    "            all_scores_res.append(non_zero_elements_list)   \n",
    "        return all_scores_res\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        base_outputs = self.base_model(input_ids)\n",
    "        lm_outputs = self.lm_head(base_outputs[0])\n",
    "        reward_outputs = self.reward_score(base_outputs[0])\n",
    "        return base_outputs, lm_outputs, reward_outputs\n",
    "\n",
    "    def run_merged_model(self, input_ids):\n",
    "        base_outputs = self.base_model(input_ids)\n",
    "        lm_outputs = self.lm_head(base_outputs[0])\n",
    "        rewards = self.get_reward_scores(input_ids, base_outputs)\n",
    "        probs = self.get_log_probs(lm_outputs, input_ids)\n",
    "        return rewards, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.39it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.00it/s]\n",
      "Some weights of the model checkpoint at Qwen/Qwen2.5-Math-PRM-7B were not used when initializing Qwen2ForProcessRewardModel: {'lm_head.weight'}\n",
      "- This IS expected if you are initializing Qwen2ForProcessRewardModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Qwen2ForProcessRewardModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.85it/s]\n"
     ]
    }
   ],
   "source": [
    "base_model_path = \"../models_merged/Qwen2--qwen_7b_merged\"\n",
    "reward_model_name = \"Qwen/Qwen2.5-Math-PRM-7B\"\n",
    "causal_model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "# Load model (automatically detects and uses safetensors)\n",
    "base_model = AutoModel.from_pretrained(base_model_path).eval()\n",
    "\n",
    "reward_model = AutoModel.from_pretrained(\n",
    "    reward_model_name, \n",
    "    trust_remote_code=True,\n",
    ").eval()\n",
    "causal_model = AutoModelForCausalLM.from_pretrained(\n",
    "    causal_model_name,\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1'\n",
    "merged_model_reward = MergedModel(tokenizer=tokenizer, base_model=reward_model.model, reward_model=reward_model, \n",
    "                           causal_model=causal_model, beginning_offset=9, total_tokens=20, device=device) #29899 GB\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#     merged_model_reward = nn.DataParallel(merged_model_reward) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "merged_model_lm = MergedModel(tokenizer=tokenizer, base_model=causal_model.model, reward_model=reward_model, \n",
    "                           causal_model=causal_model, beginning_offset=9, total_tokens=20, device=device) #29899 GB\n",
    "# merged_model_lm = nn.DataParallel(merged_model_lm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:1\"\n",
    "merged_model_true = MergedModel(tokenizer=tokenizer, base_model=base_model, reward_model=reward_model, \n",
    "                           causal_model=causal_model, beginning_offset=9, total_tokens=20, device=device) #29899 GB\n",
    "# merged_model_true = nn.DataParallel(merged_model_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Model is using {len(merged_model_true.device_ids)} GPUs: {merged_model_true.device_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {\n",
    "#     \"system\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\",\n",
    "#     \"query\": \"Sue lives in a fun neighborhood.  One weekend, the neighbors decided to play a prank on Sue.  On Friday morning, the neighbors placed 18 pink plastic flamingos out on Sue's front yard.  On Saturday morning, the neighbors took back one third of the flamingos, painted them white, and put these newly painted white flamingos back out on Sue's front yard.  Then, on Sunday morning, they added another 18 pink plastic flamingos to the collection. At noon on Sunday, how many more pink plastic flamingos were out than white plastic flamingos?\",\n",
    "#     \"response\": [\n",
    "#       \"To find out how many more pink plastic flamingos were out than white plastic flamingos at noon on Sunday, we can break down the problem into steps. First, on Friday, the neighbors start with 18 pink plastic flamingos.\",\n",
    "#       \"On Saturday, they take back one third of the flamingos. Since there were 18 flamingos, (1/3 \\\\times 18 = 6) flamingos are taken back. So, they have (18 - 6 = 12) flamingos left in their possession. Then, they paint these 6 flamingos white and put them back out on Sue's front yard. Now, Sue has the original 12 pink flamingos plus the 6 new white ones. Thus, by the end of Saturday, Sue has (12 + 6 = 18) pink flamingos and 6 white flamingos.\",\n",
    "#       \"On Sunday, the neighbors add another 18 pink plastic flamingos to Sue's front yard. By the end of Sunday morning, Sue has (18 + 18 = 36) pink flamingos and still 6 white flamingos.\",\n",
    "#       \"To find the difference, subtract the number of white flamingos from the number of pink flamingos: (36 - 6 = 30). Therefore, at noon on Sunday, there were 30 more pink plastic flamingos out than white plastic flamingos. The answer is (\\\\boxed{30}).\"\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": data['system']},\n",
    "#     {\"role\": \"user\", \"content\": data['query']},\n",
    "#     {\"role\": \"assistant\", \"content\": \"<extra_0>\"},\n",
    "# ]\n",
    "# conversation_str = tokenizer.apply_chat_template(\n",
    "#     messages, \n",
    "#     tokenize=False, \n",
    "#     add_generation_prompt=False\n",
    "# )\n",
    "\n",
    "# input_ids = tokenizer.encode(\n",
    "#     conversation_str, \n",
    "#     return_tensors=\"pt\", \n",
    "# ).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# merged_model_outputs = merged_model_true(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 152, 3584])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model_outputs[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 152, 152064]), torch.Size([1, 152, 2]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model_outputs[1].shape,merged_model_outputs[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = merged_model.get_reward_scores(input_ids, merged_model_outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9999293088912964]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(\n",
    "    conversation_str, \n",
    "    return_tensors=\"pt\", \n",
    ").to('cuda:5')\n",
    "full_outputs = merged_model.run_merged_model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9999293088912964]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_outputs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model_lm_outputs = merged_model_lm.run_merged_model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-37.7755)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.log(torch.tensor(merged_model_lm_outputs[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-74.7696)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.log(torch.tensor(full_outputs[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model_outputs = merged_model.run_merged_model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.987575113773346]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04701383039355278,\n",
       " 0.9998300075531006,\n",
       " 0.9999212026596069,\n",
       " 0.007626536302268505,\n",
       " 0.3927419185638428,\n",
       " 0.0262359119951725,\n",
       " 0.13296392560005188,\n",
       " 0.17616400122642517,\n",
       " 0.9989714622497559,\n",
       " 0.999958872795105,\n",
       " 0.0020326192025095224,\n",
       " 0.9732446670532227,\n",
       " 3.063670703795651e-07,\n",
       " 0.9358073472976685,\n",
       " 0.007398264016956091,\n",
       " 0.05035797879099846,\n",
       " 0.9429806470870972,\n",
       " 0.20274925231933594,\n",
       " 0.037123166024684906,\n",
       " 0.00041125129791907966]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model_outputs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_path = \"/data/user_data/mert/spec/search-and-learn/data_iclr/beam_search/AMead10Llama-3.2-3B-Instruct-AWQ-_n4_b0.1_q500_period0_model_len10000_1.jsonl\"\n",
    "# Read the JSON file\n",
    "with open(file_path, 'r') as f:\n",
    "    data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:44<00:00,  3.04it/s]\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(data))):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\"},\n",
    "        {\"role\": \"user\", \"content\": data[i]['problem']},\n",
    "        {\"role\": \"assistant\", \"content\": data[i]['solution']+\"<extra_0>\"},\n",
    "    ]\n",
    "    conversation_str = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    input_ids = tokenizer.encode(\n",
    "    conversation_str, \n",
    "    return_tensors=\"pt\", \n",
    "    ).to(device)\n",
    "    merged_model_outputs = merged_model.run_merged_model(input_ids)\n",
    "    score = merged_model_outputs[0][0][0]\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reward model original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6182246066136285\n",
      "0.3414839653945324\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(scores))\n",
    "print(np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reward model for merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8356026131510734\n",
      "0.11564540849264689\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(scores))\n",
    "print(np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:18<00:00,  6.38it/s]\n"
     ]
    }
   ],
   "source": [
    "scores_of_answers = []\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(data))):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\"},\n",
    "        {\"role\": \"user\", \"content\": data[i]['problem']},\n",
    "        {\"role\": \"assistant\", \"content\": data[i]['answer']+\"<extra_0>\"},\n",
    "    ]\n",
    "    conversation_str = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    input_ids = tokenizer.encode(\n",
    "    conversation_str, \n",
    "    return_tensors=\"pt\", \n",
    "    ).to(device)\n",
    "    merged_model_outputs = merged_model.run_merged_model(input_ids)\n",
    "    score = merged_model_outputs[0][0][0]\n",
    "    scores_of_answers.append(score)\n",
    "    del merged_model_outputs\n",
    "    del input_ids\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original reward model rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32676101229246707\n",
      "0.2880349981034212\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(scores_of_answers))\n",
    "print(np.std(scores_of_answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merged model rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6660591802299023\n",
      "0.14266165240892736\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(scores_of_answers))\n",
    "print(np.std(scores_of_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m merged_model_lm\u001b[38;5;241m.\u001b[39mdevice, \u001b[43minput_ids\u001b[49m\u001b[38;5;241m.\u001b[39mdevice\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_ids' is not defined"
     ]
    }
   ],
   "source": [
    "merged_model_lm.device, input_ids.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(merged_model_lm.base_model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(merged_model_lm.reward_score.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(merged_model_lm.lm_head.parameters()).device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LM Model Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:10<00:00,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5966.028393911883\n",
      "6.5761782934714574e-06\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "probs = []\n",
    "# Takes about 30Gb at A100\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(data))):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\"},\n",
    "        {\"role\": \"user\", \"content\": data[i]['problem']},\n",
    "        {\"role\": \"assistant\", \"content\": data[i]['solution']+\"<extra_0>\"},\n",
    "    ]\n",
    "    conversation_str = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    input_ids = tokenizer.encode(\n",
    "    conversation_str, \n",
    "    return_tensors=\"pt\"\n",
    "    ).to(merged_model_lm.device)\n",
    "    merged_model_outputs = merged_model_lm.run_merged_model(input_ids)\n",
    "    prob = merged_model_outputs[1][0]\n",
    "    probs.append(prob)\n",
    "    del input_ids\n",
    "    del merged_model_outputs\n",
    "    torch.cuda.empty_cache() \n",
    "print(np.sum(np.log(np.array(probs))))\n",
    "print(np.mean(probs))\n",
    "print(np.std(probs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5966.028393911882"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(probs[0])*500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Merged Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-59.66028393911881\n",
      "6.5761782934714574e-06\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "probs = []\n",
    "# Takes about 30Gb at A100\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(5)):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\"},\n",
    "        {\"role\": \"user\", \"content\": data[i]['problem']},\n",
    "        {\"role\": \"assistant\", \"content\": data[i]['solution']+\"<extra_0>\"},\n",
    "    ]\n",
    "    conversation_str = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    input_ids = tokenizer.encode(\n",
    "    conversation_str, \n",
    "    return_tensors=\"pt\"\n",
    "    ).to(merged_model_true.device)\n",
    "    merged_model_outputs = merged_model_true.run_merged_model(input_ids)\n",
    "    prob = merged_model_outputs[1][0]\n",
    "    probs.append(prob)\n",
    "    del input_ids\n",
    "    del merged_model_outputs\n",
    "    torch.cuda.empty_cache() \n",
    "print(np.sum(np.log(np.array(probs))))\n",
    "print(np.mean(probs))\n",
    "print(np.std(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(\n",
    "    conversation_str, \n",
    "    return_tensors=\"pt\"\n",
    "    ).to(merged_model_true.device)\n",
    "base_outputs = merged_model_true.base_model(input_ids)\n",
    "lm_outputs = merged_model_true.lm_head(base_outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.5761782934714574e-06\n",
      "6.5761782934714574e-06\n"
     ]
    }
   ],
   "source": [
    "inner_probs = []\n",
    "lm_prob_dist = F.softmax(lm_outputs, dim=-1)\n",
    "for offset in range(1, 3):\n",
    "    prob = lm_prob_dist[0,-offset-2,input_ids[0][-offset]].item()\n",
    "    print(prob)\n",
    "    inner_probs.append(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(447.0003, device='cuda:1', grad_fn=<SumBackward0>),\n",
       " torch.Size([1, 447, 152064]),\n",
       " torch.Size([1, 447]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_prob_dist.sum(), lm_prob_dist.shape, lm_prob_dist.argmax(dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 447, 3584])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 447, 152064])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]]], device='cuda:1', grad_fn=<ViewBackward0>),\n",
       " torch.Size([1, 447, 2]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_outputs = merged_model_true.reward_score(base_outputs[0])\n",
    "reward_outputs, reward_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=1), device(type='cuda', index=1))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_model.device, base_outputs[0].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]]], device='cuda:1', grad_fn=<ViewBackward0>),\n",
       " torch.Size([1, 447, 2]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_outputs = reward_model.score(base_outputs[0])\n",
    "reward_outputs, reward_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:1',\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[6.5762e-06, 6.5762e-06, 6.5762e-06,  ..., 6.5762e-06,\n",
       "          6.5762e-06, 6.5762e-06],\n",
       "         [6.5762e-06, 6.5762e-06, 6.5762e-06,  ..., 6.5762e-06,\n",
       "          6.5762e-06, 6.5762e-06],\n",
       "         [6.5762e-06, 6.5762e-06, 6.5762e-06,  ..., 6.5762e-06,\n",
       "          6.5762e-06, 6.5762e-06],\n",
       "         ...,\n",
       "         [6.5762e-06, 6.5762e-06, 6.5762e-06,  ..., 6.5762e-06,\n",
       "          6.5762e-06, 6.5762e-06],\n",
       "         [6.5762e-06, 6.5762e-06, 6.5762e-06,  ..., 6.5762e-06,\n",
       "          6.5762e-06, 6.5762e-06],\n",
       "         [6.5762e-06, 6.5762e-06, 6.5762e-06,  ..., 6.5762e-06,\n",
       "          6.5762e-06, 6.5762e-06]]], device='cuda:1',\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_prob_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.5761782934714574e-06, 6.5761782934714574e-06, 6.5761782934714574e-06]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inner_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([447]), torch.Size([1, 447, 152064]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0].shape, lm_prob_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answers Statistics:\n",
      "                        Model  Average PRM score  Std of PRM scores\n",
      "0  qwen (merged) math answers              0.666              0.143\n",
      "1             mistral answers              0.401              0.182\n",
      "2            deepseek answers              0.648              0.309\n",
      "\n",
      "Solutions Statistics:\n",
      "                          Model  Average PRM score  Std of PRM scores\n",
      "0  qwen (merged) math solutions              0.836              0.116\n",
      "1             mistral solutions              0.634              0.213\n",
      "2            deepseek solutions              0.481              0.348\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the statistics for answers with 3 decimal points\n",
    "stats_answers = {\n",
    "    \"Model\": [\"qwen (merged) math answers\", \"mistral answers\", \"deepseek answers\"],\n",
    "    \"Average PRM score\": [0.666, 0.401, 0.648],\n",
    "    \"Std of PRM scores\": [0.143, 0.182, 0.309]\n",
    "}\n",
    "\n",
    "# Define the statistics for solutions with 3 decimal points\n",
    "stats_solutions = {\n",
    "    \"Model\": [\"qwen (merged) math solutions\", \"mistral solutions\", \"deepseek solutions\"],\n",
    "    \"Average PRM score\": [0.836, 0.634, 0.481],\n",
    "    \"Std of PRM scores\": [0.116, 0.213, 0.348]\n",
    "}\n",
    "\n",
    "# Create DataFrames\n",
    "df_stats_answers = pd.DataFrame(stats_answers)\n",
    "df_stats_solutions = pd.DataFrame(stats_solutions)\n",
    "\n",
    "# Display the DataFrames\n",
    "print(\"Answers Statistics:\")\n",
    "print(df_stats_answers)\n",
    "print(\"\\nSolutions Statistics:\")\n",
    "print(df_stats_solutions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\\\theta),$ where $r > 0$ and $0 \\\\le \\\\theta < 2 \\\\pi.$'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['problem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We have that $r = \\\\sqrt{0^2 + 3^2} = 3.$  Also, if we draw the line connecting the origin and $(0,3),$ this line makes an angle of $\\\\frac{\\\\pi}{2}$ with the positive $x$-axis.\\n\\n[asy]\\nunitsize(0.8 cm);\\n\\ndraw((-0.5,0)--(3.5,0));\\ndraw((0,-0.5)--(0,3.5));\\ndraw(arc((0,0),3,0,90),red,Arrow(6));\\n\\ndot((0,3), red);\\nlabel(\"$(0,3)$\", (0,3), W);\\ndot((3,0), red);\\n[/asy]\\n\\nTherefore, the polar coordinates are $\\\\boxed{\\\\left( 3, \\\\frac{\\\\pi}{2} \\\\right)}.$'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['solution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\left( 3, \\\\frac{\\\\pi}{2} \\\\right)'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
