{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 1\n",
      "GPU 0: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mert/miniconda3/envs/spec/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergedModel(torch.nn.Module):\n",
    "    def __init__(self, tokenizer, base_model, reward_model, causal_model, beginning_offset, total_tokens, device):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.base_model = base_model.to(device)\n",
    "        self.reward_model = reward_model\n",
    "        self.causal_model = causal_model\n",
    "        self.lm_head = causal_model.lm_head.to(device)\n",
    "        self.reward_score = reward_model.score.to(device)\n",
    "        self.beginning_offset = beginning_offset\n",
    "        self.total_tokens = total_tokens\n",
    "        self.device = device\n",
    "        \n",
    "    def get_log_probs(self, lm_outputs, input_ids):\n",
    "        probs = torch.zeros((lm_outputs.shape[0], self.total_tokens))\n",
    "        lm_prob_dist = F.softmax(lm_outputs, dim=-1)\n",
    "        \n",
    "        for i in range(probs.shape[0]):\n",
    "            for offset in range(self.beginning_offset, self.beginning_offset+self.total_tokens):\n",
    "                prob = lm_prob_dist[i,-offset-1,input_ids[0][-offset]].item()\n",
    "                probs[i, offset-self.beginning_offset] = prob\n",
    "        return probs\n",
    "    \n",
    "    def get_reward_scores(self, input_ids, base_outputs):\n",
    "        reward_outputs = self.reward_score(base_outputs[0])\n",
    "        step_sep_id = self.tokenizer.encode(\"<extra_0>\")[0]\n",
    "        token_masks = (input_ids == step_sep_id)\n",
    "        # print(f\"Token Masks: {token_masks.shape}\")\n",
    "        logits = reward_outputs[0]\n",
    "        # print(f\"Logits: {logits.shape}\")\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        probabilities = probabilities * token_masks.unsqueeze(-1) # bs, seq_len, num_labels\n",
    "        # print(f\"Probabilities: {probabilities.shape}\")\n",
    "        all_scores_res = torch.zeros(probabilities.size(0))\n",
    "        for i in range(probabilities.shape[0]):\n",
    "            # for j in range(probabilities.shape[1]):\n",
    "                sample = probabilities[i] # seq_len, num_labels\n",
    "                positive_probs = sample[sample != 0].view(-1, 2)[:, 1] # valid_tokens, num_labels]\n",
    "                # print(f\"Positive Probs: {positive_probs.shape}\")\n",
    "                # print(f\"Positive Probs: {positive_probs.shape}\")\n",
    "                # non_zero_elements_list = positive_probs.cpu().tolist()\n",
    "                # print(f\"Non Zero Elements List: {non_zero_elements_list}\")\n",
    "                all_scores_res[i] = torch.Tensor(positive_probs)  \n",
    "        # print(f\"All Scores Res: {all_scores_res.shape}\")\n",
    "        # print(f\"All Scores Res: {all_scores_res}\")\n",
    "        return all_scores_res\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        base_outputs = self.base_model(input_ids)\n",
    "        lm_outputs = self.lm_head(base_outputs[0])\n",
    "        reward_outputs = self.reward_score(base_outputs[0])\n",
    "        return base_outputs, lm_outputs, reward_outputs\n",
    "\n",
    "    def run_merged_model(self, input_ids, tokenizer):\n",
    "        # print(f\"\\nInput Ids: {input_ids}\\n\")\n",
    "        # decoded_input_ids = tokenizer.decode(input_ids[0])\n",
    "        # print(f\"\\nDecoded Input Ids: {decoded_input_ids}\\n\")\n",
    "        base_outputs = self.base_model(input_ids)\n",
    "        lm_outputs = self.lm_head(base_outputs[0])\n",
    "        rewards = self.get_reward_scores(input_ids, base_outputs)\n",
    "        probs = self.get_log_probs(lm_outputs, input_ids)\n",
    "        return rewards, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "merged_model_true = MergedModel(tokenizer=tokenizer, base_model=base_model, reward_model=reward_model, \n",
    "                           causal_model=causal_model, beginning_offset=9, total_tokens=20, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2]), torch.Size([2, 20]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model_processed = merged_model_true.run_merged_model(input_ids_batch, tokenizer)\n",
    "merged_model_processed[0].shape,merged_model_processed[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2]), torch.Size([2, 20]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model_processed[0].shape,merged_model_processed[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9876, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model_processed[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0470)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model_processed[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.49s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.02it/s]\n",
      "Some weights of the model checkpoint at Qwen/Qwen2.5-Math-PRM-7B were not used when initializing Qwen2ForProcessRewardModel: {'lm_head.weight'}\n",
      "- This IS expected if you are initializing Qwen2ForProcessRewardModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Qwen2ForProcessRewardModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "base_model_path = \"../models_merged/Qwen2--qwen_7b_merged\"\n",
    "reward_model_name = \"Qwen/Qwen2.5-Math-PRM-7B\"\n",
    "causal_model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "# Load model (automatically detects and uses safetensors)\n",
    "base_model = AutoModel.from_pretrained(base_model_path).eval()\n",
    "\n",
    "reward_model = AutoModel.from_pretrained(\n",
    "    reward_model_name, \n",
    "    trust_remote_code=True,\n",
    ").eval()\n",
    "causal_model = AutoModelForCausalLM.from_pretrained(\n",
    "    causal_model_name,\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/data/user_data/mert/spec/models_merged/Qwen2--qwen_7b_merged-qwen2model/tokenizer_config.json',\n",
       " '/data/user_data/mert/spec/models_merged/Qwen2--qwen_7b_merged-qwen2model/special_tokens_map.json',\n",
       " '/data/user_data/mert/spec/models_merged/Qwen2--qwen_7b_merged-qwen2model/vocab.json',\n",
       " '/data/user_data/mert/spec/models_merged/Qwen2--qwen_7b_merged-qwen2model/merges.txt',\n",
       " '/data/user_data/mert/spec/models_merged/Qwen2--qwen_7b_merged-qwen2model/added_tokens.json',\n",
       " '/data/user_data/mert/spec/models_merged/Qwen2--qwen_7b_merged-qwen2model/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = \"/data/user_data/mert/spec/models_merged/Qwen2--qwen_7b_merged-qwen2model\"\n",
    "base_model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.text: . The language model uses a simplified attention mechanism to weigh theimportance of each word in a sentence. The attention weights are calculated using a scoring function that takes into account the context and the context itself. For a given sentence, the attention weight for each word is given by the formula:\n",
      "\n",
      "\n",
      "output.index: 0\n",
      "output.finish_reason: stop\n",
      "output.logprobs: None\n",
      "output.text: , Sumit, and submergedCities. The number of people in these three cities is given as follows:\n",
      "- Sumit has 12,345 people.\n",
      "- Subha has 10,987 people.\n",
      "- Submarines have 8,765 people.\n",
      "\n",
      "\n",
      "output.index: 1\n",
      "output.finish_reason: stop\n",
      "output.logprobs: None\n"
     ]
    }
   ],
   "source": [
    "for output in llm_outputs:\n",
    "    print(f\"output.text: {output.text}\")\n",
    "    print(f\"output.index: {output.index}\")\n",
    "    print(f\"output.finish_reason: {output.finish_reason}\")\n",
    "    print(f\"output.logprobs: {output.logprobs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1'\n",
    "merged_model_reward = MergedModel(tokenizer=tokenizer, base_model=reward_model.model, reward_model=reward_model, \n",
    "                           causal_model=causal_model, beginning_offset=9, total_tokens=20, device=device) #29899 GB\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#     merged_model_reward = nn.DataParallel(merged_model_reward) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "merged_model_lm = MergedModel(tokenizer=tokenizer, base_model=causal_model.model, reward_model=reward_model, \n",
    "                           causal_model=causal_model, beginning_offset=9, total_tokens=20, device=device) #29899 GB\n",
    "# merged_model_lm = nn.DataParallel(merged_model_lm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "merged_model_true = MergedModel(tokenizer=tokenizer, base_model=base_model, reward_model=reward_model, \n",
    "                           causal_model=causal_model, beginning_offset=9, total_tokens=20, device=device) #29899 GB\n",
    "# merged_model_true = nn.DataParallel(merged_model_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Model is using {len(merged_model_true.device_ids)} GPUs: {merged_model_true.device_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = [\n",
    "    {\n",
    "        \"system\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\",\n",
    "        \"query\": \"Sue lives in a fun neighborhood.  One weekend, the neighbors decided to play a prank on Sue.  On Friday morning, the neighbors placed 18 pink plastic flamingos out on Sue's front yard.  On Saturday morning, the neighbors took back one third of the flamingos, painted them white, and put these newly painted white flamingos back out on Sue's front yard.  Then, on Sunday morning, they added another 18 pink plastic flamingos to the collection. At noon on Sunday, how many more pink plastic flamingos were out than white plastic flamingos?\",\n",
    "        \"response\": [\n",
    "            \"To find out how many more pink plastic flamingos were out than white plastic flamingos at noon on Sunday, we can break down the problem into steps. First, on Friday, the neighbors start with 18 pink plastic flamingos.\",\n",
    "            \"On Saturday, they take back one third of the flamingos. Since there were 18 flamingos, (1/3 \\\\times 18 = 6) flamingos are taken back. So, they have (18 - 6 = 12) flamingos left in their possession. Then, they paint these 6 flamingos white and put them back out on Sue's front yard. Now, Sue has the original 12 pink flamingos plus the 6 new white ones. Thus, by the end of Saturday, Sue has (12 + 6 = 18) pink flamingos and 6 white flamingos.\",\n",
    "            \"On Sunday, the neighbors add another 18 pink plastic flamingos to Sue's front yard. By the end of Sunday morning, Sue has (18 + 18 = 36) pink flamingos and still 6 white flamingos.\",\n",
    "            \"To find the difference, subtract the number of white flamingos from the number of pink flamingos: (36 - 6 = 30). Therefore, at noon on Sunday, there were 30 more pink plastic flamingos out than white plastic flamingos. The answer is (\\\\boxed{30}).\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"system\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\",\n",
    "        \"query\": \"Hiii!\",\n",
    "        \"response\": [\n",
    "            \"Therefore, at noon on Sunday, there were 30 more pink plastic flamingos out than white plastic flamingos. The answer is (\\\\boxed{30}).\"\n",
    "        ]\n",
    "    },\n",
    "    # Add more data dictionaries here if needed\n",
    "]\n",
    "\n",
    "input_ids_batch = []\n",
    "max_length = 0\n",
    "\n",
    "# First pass to find maximum length\n",
    "for data in batch_data:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": data['system']},\n",
    "        {\"role\": \"user\", \"content\": data['query']},\n",
    "        {\"role\": \"assistant\", \"content\": \"<extra_0>\"},\n",
    "    ]\n",
    "    conversation_str = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    input_ids = tokenizer.encode(\n",
    "        conversation_str, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    max_length = max(max_length, input_ids.shape[1])\n",
    "for data in batch_data:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": data['system']},\n",
    "        {\"role\": \"user\", \"content\": data['query']},\n",
    "        {\"role\": \"assistant\", \"content\": \"<extra_0>\"},\n",
    "    ]\n",
    "    conversation_str = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    input_ids = tokenizer.encode(\n",
    "        conversation_str, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Pad sequence to max_length\n",
    "    padded_input_ids = torch.nn.functional.pad(\n",
    "        input_ids, \n",
    "        (0, max_length - input_ids.shape[1]), \n",
    "        value=tokenizer.pad_token_id\n",
    "    )\n",
    "    input_ids_batch.append(padded_input_ids)\n",
    "    # input_ids_batch.append(input_ids)\n",
    "\n",
    "input_ids_batch = torch.cat(input_ids_batch, dim=0).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# merged_model_outputs = merged_model_true(input_ids_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([2, 152, 3584]), DynamicCache())"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_model_outputs[0]), merged_model_outputs[0][0].shape, merged_model_outputs[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.7014e-02, 9.9983e-01, 9.9992e-01, 7.6268e-03, 3.9274e-01, 2.6236e-02,\n",
       "        1.3296e-01, 1.7616e-01, 9.9897e-01, 9.9996e-01, 2.0326e-03, 9.7324e-01,\n",
       "        3.0637e-07, 9.3581e-01, 7.3983e-03, 5.0358e-02, 9.4298e-01, 2.0275e-01,\n",
       "        3.7123e-02, 4.1125e-04])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model_processed[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9876, 0.9876, 0.9876,  ..., 0.9876, 0.9876, 0.9876],\n",
       "        [0.9876, 0.9876, 0.9876,  ..., 0.9876, 0.9876, 0.9876],\n",
       "        [0.9876, 0.9876, 0.9876,  ..., 0.9876, 0.9876, 0.9876],\n",
       "        ...,\n",
       "        [0.9876, 0.9876, 0.9876,  ..., 0.9876, 0.9876, 0.9876],\n",
       "        [0.9876, 0.9876, 0.9876,  ..., 0.9876, 0.9876, 0.9876],\n",
       "        [0.9876, 0.9876, 0.9876,  ..., 0.9876, 0.9876, 0.9876]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model_processed[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 152, 3584])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model_outputs[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 152, 152064]), torch.Size([1, 152, 2]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model_outputs[1].shape,merged_model_outputs[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = merged_model.get_reward_scores(input_ids, merged_model_outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9999293088912964]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(\n",
    "    conversation_str, \n",
    "    return_tensors=\"pt\", \n",
    ").to('cuda:5')\n",
    "full_outputs = merged_model.run_merged_model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9999293088912964]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_outputs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model_lm_outputs = merged_model_lm.run_merged_model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-37.7755)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.log(torch.tensor(merged_model_lm_outputs[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-74.7696)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.log(torch.tensor(full_outputs[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model_outputs = merged_model.run_merged_model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.987575113773346]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04701383039355278,\n",
       " 0.9998300075531006,\n",
       " 0.9999212026596069,\n",
       " 0.007626536302268505,\n",
       " 0.3927419185638428,\n",
       " 0.0262359119951725,\n",
       " 0.13296392560005188,\n",
       " 0.17616400122642517,\n",
       " 0.9989714622497559,\n",
       " 0.999958872795105,\n",
       " 0.0020326192025095224,\n",
       " 0.9732446670532227,\n",
       " 3.063670703795651e-07,\n",
       " 0.9358073472976685,\n",
       " 0.007398264016956091,\n",
       " 0.05035797879099846,\n",
       " 0.9429806470870972,\n",
       " 0.20274925231933594,\n",
       " 0.037123166024684906,\n",
       " 0.00041125129791907966]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model_outputs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_path = \"/data/user_data/mert/spec/search-and-learn/data_iclr/beam_search/AMead10Llama-3.2-3B-Instruct-AWQ-_n4_b0.1_q500_period0_model_len10000_1.jsonl\"\n",
    "# Read the JSON file\n",
    "with open(file_path, 'r') as f:\n",
    "    data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:44<00:00,  3.04it/s]\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(data))):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\"},\n",
    "        {\"role\": \"user\", \"content\": data[i]['problem']},\n",
    "        {\"role\": \"assistant\", \"content\": data[i]['solution']+\"<extra_0>\"},\n",
    "    ]\n",
    "    conversation_str = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    input_ids = tokenizer.encode(\n",
    "    conversation_str, \n",
    "    return_tensors=\"pt\", \n",
    "    ).to(device)\n",
    "    merged_model_outputs = merged_model.run_merged_model(input_ids)\n",
    "    score = merged_model_outputs[0][0][0]\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reward model original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6182246066136285\n",
      "0.3414839653945324\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(scores))\n",
    "print(np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reward model for merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8356026131510734\n",
      "0.11564540849264689\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(scores))\n",
    "print(np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:18<00:00,  6.38it/s]\n"
     ]
    }
   ],
   "source": [
    "scores_of_answers = []\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(data))):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\"},\n",
    "        {\"role\": \"user\", \"content\": data[i]['problem']},\n",
    "        {\"role\": \"assistant\", \"content\": data[i]['answer']+\"<extra_0>\"},\n",
    "    ]\n",
    "    conversation_str = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    input_ids = tokenizer.encode(\n",
    "    conversation_str, \n",
    "    return_tensors=\"pt\", \n",
    "    ).to(device)\n",
    "    merged_model_outputs = merged_model.run_merged_model(input_ids)\n",
    "    score = merged_model_outputs[0][0][0]\n",
    "    scores_of_answers.append(score)\n",
    "    del merged_model_outputs\n",
    "    del input_ids\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original reward model rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32676101229246707\n",
      "0.2880349981034212\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(scores_of_answers))\n",
    "print(np.std(scores_of_answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merged model rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6660591802299023\n",
      "0.14266165240892736\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(scores_of_answers))\n",
    "print(np.std(scores_of_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m merged_model_lm\u001b[38;5;241m.\u001b[39mdevice, \u001b[43minput_ids\u001b[49m\u001b[38;5;241m.\u001b[39mdevice\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_ids' is not defined"
     ]
    }
   ],
   "source": [
    "merged_model_lm.device, input_ids.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(merged_model_lm.base_model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(merged_model_lm.reward_score.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(merged_model_lm.lm_head.parameters()).device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LM Model Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:53<00:00,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-272.3211508501203\n",
      "0.818891541997451\n",
      "0.3202877268802231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "probs = []\n",
    "# Takes about 30Gb at A100\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(data))):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\"},\n",
    "        {\"role\": \"user\", \"content\": data[i]['problem']},\n",
    "        {\"role\": \"assistant\", \"content\": data[i]['solution']+\"<extra_0>\"},\n",
    "    ]\n",
    "    conversation_str = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    input_ids = tokenizer.encode(\n",
    "    conversation_str, \n",
    "    return_tensors=\"pt\"\n",
    "    ).to(merged_model_lm.device)\n",
    "    merged_model_outputs = merged_model_lm.run_merged_model(input_ids)\n",
    "    prob = merged_model_outputs[1][0]\n",
    "    probs.append(prob)\n",
    "    del input_ids\n",
    "    del merged_model_outputs\n",
    "    torch.cuda.empty_cache() \n",
    "print(np.sum(np.log(np.array(probs))))\n",
    "print(np.mean(probs))\n",
    "print(np.std(probs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.999998927116394,\n",
       " 0.9999986886978149,\n",
       " 0.9989515542984009,\n",
       " 0.7879471182823181,\n",
       " 0.999971866607666]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5966.028393911882"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(probs[0])*500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Merged Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:52<00:00,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-597.4337041172458\n",
      "0.6108333934515973\n",
      "0.36961618671988233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "probs = []\n",
    "# Takes about 30Gb at A100\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(data))):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\"},\n",
    "        {\"role\": \"user\", \"content\": data[i]['problem']},\n",
    "        {\"role\": \"assistant\", \"content\": data[i]['solution']+\"<extra_0>\"},\n",
    "    ]\n",
    "    conversation_str = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    input_ids = tokenizer.encode(\n",
    "    conversation_str, \n",
    "    return_tensors=\"pt\"\n",
    "    ).to(merged_model_true.device)\n",
    "    merged_model_outputs = merged_model_true.run_merged_model(input_ids)\n",
    "    prob = merged_model_outputs[1][0]\n",
    "    probs.append(prob)\n",
    "    del input_ids\n",
    "    del merged_model_outputs\n",
    "    torch.cuda.empty_cache() \n",
    "print(np.sum(np.log(np.array(probs))))\n",
    "print(np.mean(probs))\n",
    "print(np.std(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9789100289344788,\n",
       " 0.9739071726799011,\n",
       " 0.02393503300845623,\n",
       " 0.6228463649749756,\n",
       " 0.8696830868721008]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(447.0003, device='cuda:1', grad_fn=<SumBackward0>),\n",
       " torch.Size([1, 447, 152064]),\n",
       " torch.Size([1, 447]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_prob_dist.sum(), lm_prob_dist.shape, lm_prob_dist.argmax(dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 447, 3584])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 447, 152064])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", publicpr coupleess to Grey Kenectdro show< ARE_0>ナル\n",
      "tensor([[  5501,   2874,   3019,    553,   3019,     11,    323,   2182,    697,\n",
      "           1590,   4226,   2878,    220,    196,   5131,    291,   6257,  15757,\n",
      "             91,     68,    354,    842,     91,   1784,     91,   2468,   8757,\n",
      "            842,     91,     29,    872,     27,     91,    408,   8757,    842,\n",
      "             91,     29,   4710,   7169,    279,   1459,   4930,     15,     11,\n",
      "             18,  15087,    304,  51424,  13934,    311,  24660,  13934,     13,\n",
      "            220,  11252,    697,   4226,    304,    279,   1352,   4930,     81,\n",
      "             11,   9598,   1915,  98406,   1380,    400,     81,    861,    220,\n",
      "             15,      3,    323,    400,     15,   1124,    273,    220,   9598,\n",
      "           1915,    366,    220,     17,   1124,   2493,   2418,     27,     91,\n",
      "             68,    354,    842,     91,   1784,     91,   2468,   8757,    842,\n",
      "             91,     29,  77091,     27,     91,    408,   8757,    842,     91,\n",
      "             29,   4710,  16246,    279,   1459,   4930,     15,     11,     18,\n",
      "          15087,    304,  51424,  13934,     11,    582,    646,   5508,    432,\n",
      "            311,  24660,  13934,    438,  11017,   1447, 151651,     27,     91,\n",
      "             68,    354,    842,     91,     29]])\n"
     ]
    }
   ],
   "source": [
    "trial_tensor = torch.tensor([11,    584,    649,   5625,    433,    311,  25685,  14259,    439,\n",
    "          11263,   1473,     27,  15824,     62,     15,     29, 128009], dtype=torch.long)\n",
    "decoded_trial = tokenizer.decode(trial_tensor.tolist())\n",
    "print(decoded_trial)\n",
    "real_string = \"Please reason step by step, and put your final answer within \\boxed{}.<|eot_id|><|start_header_id|>user<|end_header_id|> \\n\\n Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\theta),$ where $r > 0$ and $0 \\le \\theta < 2 \\pi.$<|eot_id|><|start_header_id|>assistant<|end_header_id|> \\n\\n Given the point $(0,3)$ in rectangular coordinates, we can convert it to polar coordinates as follows:\\n\\n<extra_0><|eot_id|>\"\n",
    "real_token_ids = tokenizer.encode(real_string, return_tensors=\"pt\")\n",
    "print(real_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answers Statistics:\n",
      "                        Model  Average PRM score  Std of PRM scores\n",
      "0  qwen (merged) math answers              0.666              0.143\n",
      "1             mistral answers              0.401              0.182\n",
      "2            deepseek answers              0.648              0.309\n",
      "\n",
      "Solutions Statistics:\n",
      "                          Model  Average PRM score  Std of PRM scores\n",
      "0  qwen (merged) math solutions              0.836              0.116\n",
      "1             mistral solutions              0.634              0.213\n",
      "2            deepseek solutions              0.481              0.348\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the statistics for answers with 3 decimal points\n",
    "stats_answers = {\n",
    "    \"Model\": [\"qwen (merged) math answers\", \"mistral answers\", \"deepseek answers\"],\n",
    "    \"Average PRM score\": [0.666, 0.401, 0.648],\n",
    "    \"Std of PRM scores\": [0.143, 0.182, 0.309]\n",
    "}\n",
    "\n",
    "# Define the statistics for solutions with 3 decimal points\n",
    "stats_solutions = {\n",
    "    \"Model\": [\"qwen (merged) math solutions\", \"mistral solutions\", \"deepseek solutions\"],\n",
    "    \"Average PRM score\": [0.836, 0.634, 0.481],\n",
    "    \"Std of PRM scores\": [0.116, 0.213, 0.348]\n",
    "}\n",
    "\n",
    "# Create DataFrames\n",
    "df_stats_answers = pd.DataFrame(stats_answers)\n",
    "df_stats_solutions = pd.DataFrame(stats_solutions)\n",
    "\n",
    "# Display the DataFrames\n",
    "print(\"Answers Statistics:\")\n",
    "print(df_stats_answers)\n",
    "print(\"\\nSolutions Statistics:\")\n",
    "print(df_stats_solutions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\\\theta),$ where $r > 0$ and $0 \\\\le \\\\theta < 2 \\\\pi.$'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['problem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We have that $r = \\\\sqrt{0^2 + 3^2} = 3.$  Also, if we draw the line connecting the origin and $(0,3),$ this line makes an angle of $\\\\frac{\\\\pi}{2}$ with the positive $x$-axis.\\n\\n[asy]\\nunitsize(0.8 cm);\\n\\ndraw((-0.5,0)--(3.5,0));\\ndraw((0,-0.5)--(0,3.5));\\ndraw(arc((0,0),3,0,90),red,Arrow(6));\\n\\ndot((0,3), red);\\nlabel(\"$(0,3)$\", (0,3), W);\\ndot((3,0), red);\\n[/asy]\\n\\nTherefore, the polar coordinates are $\\\\boxed{\\\\left( 3, \\\\frac{\\\\pi}{2} \\\\right)}.$'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['solution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\left( 3, \\\\frac{\\\\pi}{2} \\\\right)'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
