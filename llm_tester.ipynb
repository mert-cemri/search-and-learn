{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# import torch\n",
    "# import transformers\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# bnb_config = transformers.BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "# )\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     device_map=\"auto\",\n",
    "#     quantization_config=bnb_config,\n",
    "#     token=\"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\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 02:13:38.533110: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1735006418.548298 1582907 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1735006418.552845 1582907 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-24 02:13:38.570850: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/mert/.conda/envs/spec/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sal.config import Config\n",
    "from sal.models.reward_models import PRM\n",
    "from src.sal.search.utils import Beam, build_conv, generate_k_steps, last\n",
    "from sal.utils.parser import H4ArgumentParser\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-24 02:13:56 arg_utils.py:953] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 12-24 02:13:56 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 12-24 02:13:56 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='unsloth/Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='unsloth/Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=unsloth/Llama-3.2-1B-Instruct, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 12-24 02:14:00 model_runner.py:1060] Starting to load model unsloth/Llama-3.2-1B-Instruct...\n",
      "INFO 12-24 02:14:01 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 12-24 02:14:01 weight_utils.py:288] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2718e26e84054299b4e210da63bd31f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-24 02:14:02 model_runner.py:1071] Loading model weights took 2.3185 GB\n",
      "------\n",
      "\n",
      "\n",
      "1\n",
      "1\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "2\n",
      "2\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "3\n",
      "3\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "4\n",
      "4\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "5\n",
      "5\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "6\n",
      "6\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "7\n",
      "7\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "8\n",
      "8\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "9\n",
      "9\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "10\n",
      "10\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "11\n",
      "11\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "12\n",
      "12\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "13\n",
      "13\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "14\n",
      "14\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "15\n",
      "15\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "16\n",
      "16\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "17\n",
      "17\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "18\n",
      "18\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "19\n",
      "19\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "20\n",
      "20\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "21\n",
      "21\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "22\n",
      "22\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "23\n",
      "23\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "24\n",
      "24\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "25\n",
      "25\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "26\n",
      "26\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "27\n",
      "27\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "28\n",
      "28\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "29\n",
      "29\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "30\n",
      "30\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "31\n",
      "31\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "32\n",
      "32\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "33\n",
      "33\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "34\n",
      "34\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "35\n",
      "35\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "36\n",
      "36\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "37\n",
      "37\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "38\n",
      "38\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "39\n",
      "39\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "40\n",
      "40\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "41\n",
      "41\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "42\n",
      "42\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "43\n",
      "43\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "44\n",
      "44\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "45\n",
      "45\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "46\n",
      "46\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "47\n",
      "47\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "48\n",
      "48\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "49\n",
      "49\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "50\n",
      "50\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "51\n",
      "51\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "52\n",
      "52\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "53\n",
      "53\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "54\n",
      "54\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "55\n",
      "55\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "56\n",
      "56\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "57\n",
      "57\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "58\n",
      "58\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "59\n",
      "59\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "60\n",
      "60\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "61\n",
      "61\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "62\n",
      "62\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "63\n",
      "63\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "64\n",
      "64\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "65\n",
      "65\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "66\n",
      "66\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "67\n",
      "67\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "68\n",
      "68\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "69\n",
      "69\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "70\n",
      "70\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "71\n",
      "71\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "72\n",
      "72\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "73\n",
      "73\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "74\n",
      "74\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "75\n",
      "75\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "76\n",
      "76\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "77\n",
      "77\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "78\n",
      "78\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "79\n",
      "79\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "80\n",
      "80\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "81\n",
      "81\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "82\n",
      "82\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "83\n",
      "83\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "84\n",
      "84\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "85\n",
      "85\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "86\n",
      "86\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "87\n",
      "87\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "88\n",
      "88\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "89\n",
      "89\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "90\n",
      "90\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "91\n",
      "91\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "92\n",
      "92\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "93\n",
      "93\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "94\n",
      "94\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "95\n",
      "95\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "96\n",
      "96\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "97\n",
      "97\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "98\n",
      "98\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "99\n",
      "99\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "100\n",
      "100\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "101\n",
      "101\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "102\n",
      "102\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "103\n",
      "103\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "104\n",
      "104\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "105\n",
      "105\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "106\n",
      "106\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "107\n",
      "107\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "108\n",
      "108\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "109\n",
      "109\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "110\n",
      "110\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "111\n",
      "111\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "112\n",
      "112\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "113\n",
      "113\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "114\n",
      "114\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "115\n",
      "115\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "116\n",
      "116\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "117\n",
      "117\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "118\n",
      "118\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "119\n",
      "119\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "120\n",
      "120\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "121\n",
      "121\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "122\n",
      "122\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "123\n",
      "123\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "124\n",
      "124\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "125\n",
      "125\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "126\n",
      "126\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "127\n",
      "127\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "128\n",
      "128\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "129\n",
      "129\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "130\n",
      "130\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "131\n",
      "131\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "132\n",
      "132\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "133\n",
      "133\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "134\n",
      "134\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "135\n",
      "135\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "136\n",
      "136\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "137\n",
      "137\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "138\n",
      "138\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "139\n",
      "139\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "140\n",
      "140\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "141\n",
      "141\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "142\n",
      "142\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "143\n",
      "143\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "144\n",
      "144\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "145\n",
      "145\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "146\n",
      "146\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "147\n",
      "147\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "148\n",
      "148\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "149\n",
      "149\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "150\n",
      "150\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "151\n",
      "151\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "152\n",
      "152\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "153\n",
      "153\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "154\n",
      "154\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "155\n",
      "155\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "156\n",
      "156\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "157\n",
      "157\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "158\n",
      "158\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "159\n",
      "159\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "160\n",
      "160\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "161\n",
      "161\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "162\n",
      "162\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "163\n",
      "163\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "164\n",
      "164\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "165\n",
      "165\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "166\n",
      "166\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "167\n",
      "167\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "168\n",
      "168\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "169\n",
      "169\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "170\n",
      "170\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "171\n",
      "171\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "172\n",
      "172\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "173\n",
      "173\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "174\n",
      "174\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "175\n",
      "175\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "176\n",
      "176\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "177\n",
      "177\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "178\n",
      "178\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "179\n",
      "179\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "180\n",
      "180\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "181\n",
      "181\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "182\n",
      "182\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "183\n",
      "183\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "184\n",
      "184\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "185\n",
      "185\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "186\n",
      "186\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "187\n",
      "187\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "188\n",
      "188\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "189\n",
      "189\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "190\n",
      "190\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "191\n",
      "191\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "192\n",
      "192\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "193\n",
      "193\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "194\n",
      "194\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "195\n",
      "195\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "196\n",
      "196\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "197\n",
      "197\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "198\n",
      "198\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "199\n",
      "199\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "200\n",
      "200\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "201\n",
      "201\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "202\n",
      "202\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "203\n",
      "203\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "204\n",
      "204\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "205\n",
      "205\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "206\n",
      "206\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "207\n",
      "207\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "208\n",
      "208\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "209\n",
      "209\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "210\n",
      "210\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "211\n",
      "211\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "212\n",
      "212\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "213\n",
      "213\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "214\n",
      "214\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "215\n",
      "215\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "216\n",
      "216\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "217\n",
      "217\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "218\n",
      "218\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "219\n",
      "219\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "220\n",
      "220\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "221\n",
      "221\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "222\n",
      "222\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "223\n",
      "223\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "224\n",
      "224\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "225\n",
      "225\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "226\n",
      "226\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "227\n",
      "227\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "228\n",
      "228\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "229\n",
      "229\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "230\n",
      "230\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "231\n",
      "231\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "232\n",
      "232\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "233\n",
      "233\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "234\n",
      "234\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "235\n",
      "235\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "236\n",
      "236\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "237\n",
      "237\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "238\n",
      "238\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "239\n",
      "239\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "240\n",
      "240\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "241\n",
      "241\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "242\n",
      "242\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "243\n",
      "243\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "244\n",
      "244\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "245\n",
      "245\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "246\n",
      "246\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "247\n",
      "247\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "248\n",
      "248\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "249\n",
      "249\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "250\n",
      "250\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "251\n",
      "251\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "252\n",
      "252\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "253\n",
      "253\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "254\n",
      "254\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "255\n",
      "255\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "256\n",
      "256\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "INFO 12-24 02:14:05 gpu_executor.py:122] # GPU blocks: 28971, # CPU blocks: 8192\n",
      "INFO 12-24 02:14:05 gpu_executor.py:126] Maximum concurrency for 131072 tokens per request: 3.54x\n",
      "INFO 12-24 02:14:18 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-24 02:14:18 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-24 02:15:01 model_runner.py:1530] Graph capturing finished in 43 secs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "num_gpus = torch.cuda.device_count()\n",
    "llm = LLM(\n",
    "        model='unsloth/Llama-3.2-1B-Instruct',\n",
    "        gpu_memory_utilization=0.4,\n",
    "        enable_prefix_caching=True,\n",
    "        seed=42,\n",
    "        tensor_parallel_size=num_gpus,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-24 01:16:17 config.py:1670] Downcasting torch.float32 to torch.float16.\n",
      "INFO 12-24 01:16:24 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='allenai/OLMo-1B-hf', speculative_config=None, tokenizer='allenai/OLMo-1B-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=allenai/OLMo-1B-hf, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 12-24 01:16:25 model_runner.py:1060] Starting to load model allenai/OLMo-1B-hf...\n",
      "INFO 12-24 01:16:27 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 12-24 01:16:27 weight_utils.py:288] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f7c54a2aaf4bf79fa46164ce814b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-24 01:16:29 model_runner.py:1071] Loading model weights took 2.1924 GB\n",
      "INFO 12-24 01:16:30 gpu_executor.py:122] # GPU blocks: 7675, # CPU blocks: 2048\n",
      "INFO 12-24 01:16:30 gpu_executor.py:126] Maximum concurrency for 2048 tokens per request: 59.96x\n",
      "INFO 12-24 01:16:43 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-24 01:16:43 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-24 01:17:45 model_runner.py:1530] Graph capturing finished in 63 secs.\n"
     ]
    }
   ],
   "source": [
    "llm2 = LLM(\n",
    "        model='allenai/OLMo-1B-hf',\n",
    "        gpu_memory_utilization=0.4,\n",
    "        enable_prefix_caching=True,\n",
    "        seed=42,\n",
    "        tensor_parallel_size=num_gpus,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'~profileaphanaelopenid'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([93, 5478, 64, 10146, 64, 301, 49651])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "                temperature=2,\n",
    "                max_tokens=1,\n",
    "                top_p=0.7,\n",
    "                n=1,\n",
    "                prompt_logprobs=1\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ['hello it is me']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_outputs = llm.generate(prompts[0], sampling_params, use_tqdm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_outputs2 = llm.generate(prompts[0], sampling_params, use_tqdm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello it is me'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_outputs[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " {15339: Logprob(logprob=-10.785621643066406, rank=8248, decoded_token='hello'),\n",
       "  2: Logprob(logprob=-8.094215393066406, rank=1, decoded_token='#')},\n",
       " {433: Logprob(logprob=-8.240577697753906, rank=136, decoded_token=' it'),\n",
       "  11: Logprob(logprob=-5.146827220916748, rank=1, decoded_token=',')},\n",
       " {374: Logprob(logprob=-3.3976590633392334, rank=2, decoded_token=' is'),\n",
       "  596: Logprob(logprob=-2.8351590633392334, rank=1, decoded_token=\"'s\")},\n",
       " {757: Logprob(logprob=-4.928860664367676, rank=2, decoded_token=' me'),\n",
       "  6555: Logprob(logprob=-4.397610664367676, rank=1, decoded_token=' nice')}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_outputs[0].prompt_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CompletionOutput(index=0, text='\\n', token_ids=(198,), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_outputs[0].outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer.encode(prompts[0], add_special_tokens=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = token_ids[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{374: Logprob(logprob=-3.3976590633392334, rank=2, decoded_token=' is'),\n",
       "  596: Logprob(logprob=-2.8351590633392334, rank=1, decoded_token=\"'s\")},\n",
       " {757: Logprob(logprob=-4.928860664367676, rank=2, decoded_token=' me'),\n",
       "  6555: Logprob(logprob=-4.397610664367676, rank=1, decoded_token=' nice')}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_outputs[0].prompt_logprobs[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[374, 757]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = -2\n",
    "total_log_probs = 0\n",
    "for key in keys:\n",
    "    total_log_probs += llm_outputs[0].prompt_logprobs[counter][key].logprob\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.32651972770691"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.928860664367676"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(llm_outputs[0].prompt_logprobs[-1].values())[0].logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ['hello it is me']\n",
    "token_ids = tokenizer.encode(prompts[0], add_special_tokens=True) \n",
    "keys = token_ids[-2:]\n",
    "counter_start = -len(keys)\n",
    "log_probs = [llm_outputs[0].prompt_logprobs[counter_start + i].get(key).logprob if llm_outputs[0].prompt_logprobs[counter_start + i].get(key) else 0.0 for i, key in enumerate(keys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-8.3265)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E1223 23:27:21.027555854 socket.cpp:957] [c10d] The client socket has timed out after 600s while trying to connect to (127.0.0.1, 48951).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=1478075)\u001b[0;0m ERROR 12-23 23:27:21 multiproc_worker_utils.py:231] Exception in worker VllmWorkerProcess while processing method init_device: The client socket has timed out after 600s while trying to connect to (127.0.0.1, 48951)., Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1478075)\u001b[0;0m ERROR 12-23 23:27:21 multiproc_worker_utils.py:231]   File \"/home/mert/.conda/envs/spec/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py\", line 224, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1478075)\u001b[0;0m ERROR 12-23 23:27:21 multiproc_worker_utils.py:231]     output = executor(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1478075)\u001b[0;0m ERROR 12-23 23:27:21 multiproc_worker_utils.py:231]   File \"/home/mert/.conda/envs/spec/lib/python3.10/site-packages/vllm/worker/worker.py\", line 176, in init_device\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1478075)\u001b[0;0m ERROR 12-23 23:27:21 multiproc_worker_utils.py:231]     init_worker_distributed_environment(self.parallel_config, self.rank,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1478075)\u001b[0;0m ERROR 12-23 23:27:21 multiproc_worker_utils.py:231]   File \"/home/mert/.conda/envs/spec/lib/python3.10/site-packages/vllm/worker/worker.py\", line 453, in init_worker_distributed_environment\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1478075)\u001b[0;0m ERROR 12-23 23:27:21 multiproc_worker_utils.py:231]     init_distributed_environment(parallel_config.world_size, rank,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1478075)\u001b[0;0m ERROR 12-23 23:27:21 multiproc_worker_utils.py:231]   File \"/home/mert/.conda/envs/spec/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\", line 938, in init_distributed_environment\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1478075)\u001b[0;0m ERROR 12-23 23:27:21 multiproc_worker_utils.py:231]     torch.distributed.init_process_group(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1478075)\u001b[0;0m ERROR 12-23 23:27:21 multiproc_worker_utils.py:231]   File \"/home/mert/.conda/envs/spec/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 79, in wrapper\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1478075)\u001b[0;0m ERROR 12-23 23:27:21 multiproc_worker_utils.py:231]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1478075)\u001b[0;0m ERROR 12-23 23:27:21 multiproc_worker_utils.py:231]   File \"/home/mert/.conda/envs/spec/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 93, in wrapper\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1478075)\u001b[0;0m ERROR 12-23 23:27:21 multiproc_worker_utils.py:231]     func_return = func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1478075)\u001b[0;0m ERROR 12-23 23:27:21 multiproc_worker_utils.py:231]   File \"/home/mert/.conda/envs/spec/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 1361, in init_process_group\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1478075)\u001b[0;0m ERROR 12-23 23:27:21 multiproc_worker_utils.py:231]     store, rank, world_size = next(rendezvous_iterator)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1478075)\u001b[0;0m ERROR 12-23 23:27:21 multiproc_worker_utils.py:231]   File \"/home/mert/.conda/envs/spec/lib/python3.10/site-packages/torch/distributed/rendezvous.py\", line 211, in _tcp_rendezvous_handler\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1478075)\u001b[0;0m ERROR 12-23 23:27:21 multiproc_worker_utils.py:231]     store = _create_c10d_store(result.hostname, result.port, rank, world_size, timeout, use_libuv)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1478075)\u001b[0;0m ERROR 12-23 23:27:21 multiproc_worker_utils.py:231]   File \"/home/mert/.conda/envs/spec/lib/python3.10/site-packages/torch/distributed/rendezvous.py\", line 185, in _create_c10d_store\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1478075)\u001b[0;0m ERROR 12-23 23:27:21 multiproc_worker_utils.py:231]     return TCPStore(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1478075)\u001b[0;0m ERROR 12-23 23:27:21 multiproc_worker_utils.py:231] torch.distributed.DistNetworkError: The client socket has timed out after 600s while trying to connect to (127.0.0.1, 48951).\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1478075)\u001b[0;0m ERROR 12-23 23:27:21 multiproc_worker_utils.py:231] \n"
     ]
    }
   ],
   "source": [
    "total_log_probs = torch.sum(torch.tensor(log_probs))\n",
    "total_log_probs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
