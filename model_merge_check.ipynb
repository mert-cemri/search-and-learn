{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mert/miniconda3/envs/spec/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import torch\n",
    "torch.cuda.device_count()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(128257, 3072, padding_idx=128256)\n",
       "  (layers): ModuleList(\n",
       "    (0-27): 28 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "        (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "        (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "  (rotary_emb): LlamaRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name_or_path = \"Ray2333/GRM-Llama3.2-3B-rewardmodel-ft\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Load model (automatically detects and uses safetensors)\n",
    "model2 = AutoModel.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Move to GPU if available\n",
    "model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n",
       "  (layers): ModuleList(\n",
       "    (0-27): 28 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "        (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "        (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "  (rotary_emb): LlamaRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"unsloth/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer3 = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load model (automatically detects and uses safetensors)\n",
    "model3 = AutoModel.from_pretrained(model_path)\n",
    "\n",
    "# Move to GPU if available\n",
    "model3.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 4/4 [05:58<00:00, 89.66s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.21it/s]\n"
     ]
    }
   ],
   "source": [
    "model_pathm = \"OpenRLHF/Llama-3-8b-rm-700k\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer4 = AutoTokenizer.from_pretrained(model_pathm)\n",
    "\n",
    "# Load model (automatically detects and uses safetensors)\n",
    "model4 = AutoModel.from_pretrained(model_pathm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaModel(\n",
      "  (embed_tokens): Embedding(128256, 4096)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 4/4 [06:22<00:00, 95.55s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.79it/s]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"OpenRLHF/Llama-3-8b-sft-mixture\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer5 = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load model (automatically detects and uses safetensors)\n",
    "model5 = AutoModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaModel(\n",
      "  (embed_tokens): Embedding(128256, 4096)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 4/4 [06:04<00:00, 91.24s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.90it/s]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"Qwen/Qwen2.5-Math-PRM-7B\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer6 = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load model (automatically detects and uses safetensors)\n",
    "model6 = AutoModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 4/4 [06:02<00:00, 90.65s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.50it/s]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer7 = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load model (automatically detects and uses safetensors)\n",
    "model7 = AutoModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2Model(\n",
      "  (embed_tokens): Embedding(152064, 3584)\n",
      "  (layers): ModuleList(\n",
      "    (0-27): 28 x Qwen2DecoderLayer(\n",
      "      (self_attn): Qwen2Attention(\n",
      "        (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "        (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "        (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "        (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "      )\n",
      "      (mlp): Qwen2MLP(\n",
      "        (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "        (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "        (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-05)\n",
      "      (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (norm): Qwen2RMSNorm((3584,), eps=1e-05)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2Model(\n",
      "  (embed_tokens): Embedding(152064, 3584)\n",
      "  (layers): ModuleList(\n",
      "    (0-27): 28 x Qwen2DecoderLayer(\n",
      "      (self_attn): Qwen2Attention(\n",
      "        (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "        (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "        (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "        (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "      )\n",
      "      (mlp): Qwen2MLP(\n",
      "        (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "        (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "        (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "      (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "    )\n",
      "  )\n",
      "  (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_step_rewards(logits, token_masks):\n",
    "    probabilities = F.softmax(logits, dim=-1)\n",
    "    probabilities = probabilities * token_masks.unsqueeze(-1) # bs, seq_len, num_labels\n",
    "    \n",
    "    all_scores_res = []\n",
    "    for i in range(probabilities.size(0)):\n",
    "        sample = probabilities[i] # seq_len, num_labels\n",
    "        positive_probs = sample[sample != 0].view(-1, 2)[:, 1] # valid_tokens, num_labels\n",
    "        non_zero_elements_list = positive_probs.cpu().tolist()\n",
    "        all_scores_res.append(non_zero_elements_list)\n",
    "    return all_scores_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"system\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\",\n",
    "    \"query\": \"Sue lives in a fun neighborhood.  One weekend, the neighbors decided to play a prank on Sue.  On Friday morning, the neighbors placed 18 pink plastic flamingos out on Sue's front yard.  On Saturday morning, the neighbors took back one third of the flamingos, painted them white, and put these newly painted white flamingos back out on Sue's front yard.  Then, on Sunday morning, they added another 18 pink plastic flamingos to the collection. At noon on Sunday, how many more pink plastic flamingos were out than white plastic flamingos?\",\n",
    "    \"response\": [\n",
    "      \"To find out how many more pink plastic flamingos were out than white plastic flamingos at noon on Sunday, we can break down the problem into steps. First, on Friday, the neighbors start with 18 pink plastic flamingos.\",\n",
    "      \"On Saturday, they take back one third of the flamingos. Since there were 18 flamingos, (1/3 \\\\times 18 = 6) flamingos are taken back. So, they have (18 - 6 = 12) flamingos left in their possession. Then, they paint these 6 flamingos white and put them back out on Sue's front yard. Now, Sue has the original 12 pink flamingos plus the 6 new white ones. Thus, by the end of Saturday, Sue has (12 + 6 = 18) pink flamingos and 6 white flamingos.\",\n",
    "      \"On Sunday, the neighbors add another 18 pink plastic flamingos to Sue's front yard. By the end of Sunday morning, Sue has (18 + 18 = 36) pink flamingos and still 6 white flamingos.\",\n",
    "      \"To find the difference, subtract the number of white flamingos from the number of pink flamingos: (36 - 6 = 30). Therefore, at noon on Sunday, there were 30 more pink plastic flamingos out than white plastic flamingos. The answer is (\\\\boxed{30}).\"\n",
    "    ]\n",
    "}\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": data['system']},\n",
    "    {\"role\": \"user\", \"content\": data['query']},\n",
    "    {\"role\": \"assistant\", \"content\": \"<extra_0>\"},\n",
    "]\n",
    "\n",
    "\n",
    "tokenizer = tokenizer6\n",
    "model = model6\n",
    "conversation_str = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=False\n",
    ")\n",
    "\n",
    "input_ids = tokenizer.encode(\n",
    "    conversation_str, \n",
    "    return_tensors=\"pt\", \n",
    ").to(model6.device)\n",
    "\n",
    "outputs = model(input_ids=input_ids)\n",
    "\n",
    "step_sep_id = tokenizer.encode(\"<extra_0>\")[0]\n",
    "token_masks = (input_ids == step_sep_id)\n",
    "\n",
    "logits = outputs[0]\n",
    "probabilities = F.softmax(logits, dim=-1)\n",
    "probabilities = probabilities * token_masks.unsqueeze(-1) # bs, seq_len, num_labels\n",
    "\n",
    "all_scores_res = []\n",
    "for i in range(probabilities.size(0)):\n",
    "    sample = probabilities[i] # seq_len, num_labels\n",
    "    positive_probs = sample[sample != 0].view(-1, 2)[:, 1] # valid_tokens, num_labels\n",
    "    non_zero_elements_list = positive_probs.cpu().tolist()\n",
    "    all_scores_res.append(non_zero_elements_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(201)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(sample[sample != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3584])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[sample != 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([152, 3584])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1792, 2])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[sample != 0].view(-1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 152, 3584]), torch.Size([1, 152, 3584]), 1)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, probabilities.shape, probabilities.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities[:,149,:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False,  True,\n",
       "         False, False]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1792"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(step_reward[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|im_start|>system\\nPlease reason step by step, and put your final answer within \\\\boxed{}.<|im_end|>\\n<|im_start|>user\\nSue lives in a fun neighborhood.  One weekend, the neighbors decided to play a prank on Sue.  On Friday morning, the neighbors placed 18 pink plastic flamingos out on Sue's front yard.  On Saturday morning, the neighbors took back one third of the flamingos, painted them white, and put these newly painted white flamingos back out on Sue's front yard.  Then, on Sunday morning, they added another 18 pink plastic flamingos to the collection. At noon on Sunday, how many more pink plastic flamingos were out than white plastic flamingos?<|im_end|>\\n<|im_start|>assistant\\n<extra_0><|im_end|><|endoftext|>\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1792"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(step_reward[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 0.8115752935409546)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.argmax(step_reward[0]), step_reward[0][np.argmax(step_reward[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' forward'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer6.decode(4637)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.12s/it]\n",
      "Some weights of the model checkpoint at Qwen/Qwen2.5-Math-PRM-7B were not used when initializing Qwen2ForProcessRewardModel: {'lm_head.weight'}\n",
      "- This IS expected if you are initializing Qwen2ForProcessRewardModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Qwen2ForProcessRewardModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9999293088912964]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-Math-PRM-7B\"\n",
    "device = \"auto\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "reward_model = AutoModel.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=device, \n",
    "    trust_remote_code=True,\n",
    ").eval()\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"system\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\",\n",
    "    \"query\": \"Sue lives in a fun neighborhood.  One weekend, the neighbors decided to play a prank on Sue.  On Friday morning, the neighbors placed 18 pink plastic flamingos out on Sue's front yard.  On Saturday morning, the neighbors took back one third of the flamingos, painted them white, and put these newly painted white flamingos back out on Sue's front yard.  Then, on Sunday morning, they added another 18 pink plastic flamingos to the collection. At noon on Sunday, how many more pink plastic flamingos were out than white plastic flamingos?\",\n",
    "    \"response\": [\n",
    "      \"To find out how many more pink plastic flamingos were out than white plastic flamingos at noon on Sunday, we can break down the problem into steps. First, on Friday, the neighbors start with 18 pink plastic flamingos.\",\n",
    "      \"On Saturday, they take back one third of the flamingos. Since there were 18 flamingos, (1/3 \\\\times 18 = 6) flamingos are taken back. So, they have (18 - 6 = 12) flamingos left in their possession. Then, they paint these 6 flamingos white and put them back out on Sue's front yard. Now, Sue has the original 12 pink flamingos plus the 6 new white ones. Thus, by the end of Saturday, Sue has (12 + 6 = 18) pink flamingos and 6 white flamingos.\",\n",
    "      \"On Sunday, the neighbors add another 18 pink plastic flamingos to Sue's front yard. By the end of Sunday morning, Sue has (18 + 18 = 36) pink flamingos and still 6 white flamingos.\",\n",
    "      \"To find the difference, subtract the number of white flamingos from the number of pink flamingos: (36 - 6 = 30). Therefore, at noon on Sunday, there were 30 more pink plastic flamingos out than white plastic flamingos. The answer is (\\\\boxed{30}).\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": data['system']},\n",
    "    {\"role\": \"user\", \"content\": data['query']},\n",
    "    {\"role\": \"assistant\", \"content\": \"<extra_0>\"},\n",
    "]\n",
    "conversation_str = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=False\n",
    ")\n",
    "\n",
    "input_ids = tokenizer.encode(\n",
    "    conversation_str, \n",
    "    return_tensors=\"pt\", \n",
    ").to(model.device)\n",
    "\n",
    "outputs = reward_model(input_ids=input_ids)\n",
    "\n",
    "step_sep_id = tokenizer.encode(\"<extra_0>\")[0]\n",
    "token_masks = (input_ids == step_sep_id)\n",
    "\n",
    "logits = outputs[0]\n",
    "probabilities = F.softmax(logits, dim=-1)\n",
    "probabilities = probabilities * token_masks.unsqueeze(-1) # bs, seq_len, num_labels\n",
    "\n",
    "all_scores_res = []\n",
    "for i in range(probabilities.size(0)):\n",
    "    sample = probabilities[i] # seq_len, num_labels\n",
    "    positive_probs = sample[sample != 0].view(-1, 2)[:, 1] # valid_tokens, num_labels\n",
    "    non_zero_elements_list = positive_probs.cpu().tolist()\n",
    "    all_scores_res.append(non_zero_elements_list)   \n",
    "step_reward = all_scores_res\n",
    "print(step_reward)  # [[1.0, 0.1904296875, 0.9765625, 1.0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 152, 2])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForProcessRewardModel(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(152064, 3584)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-05)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((3584,), eps=1e-05)\n",
      "  )\n",
      "  (score): Sequential(\n",
      "    (0): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=3584, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(reward_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2Model(\n",
      "  (embed_tokens): Embedding(152064, 3584)\n",
      "  (layers): ModuleList(\n",
      "    (0-27): 28 x Qwen2DecoderLayer(\n",
      "      (self_attn): Qwen2Attention(\n",
      "        (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "        (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "        (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "        (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "      )\n",
      "      (mlp): Qwen2MLP(\n",
      "        (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "        (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "        (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-05)\n",
      "      (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (norm): Qwen2RMSNorm((3584,), eps=1e-05)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.39s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(152064, 3584)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=3584, out_features=152064, bias=False)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=3584, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_model.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 3584])\n",
      "torch.Size([1, 10, 152064])\n",
      "torch.Size([1, 10, 2])\n"
     ]
    }
   ],
   "source": [
    "# Example usage of model.lm_head\n",
    "import torch\n",
    "\n",
    "# Create a dummy input tensor\n",
    "input_tensor = torch.randn(1, 10, model.config.hidden_size, device=model.device, dtype=model.dtype)\n",
    "\n",
    "# Pass the input tensor through the lm_head\n",
    "output_tensor = model.lm_head(input_tensor)\n",
    "reward_tensor = reward_model.score(input_tensor)\n",
    "\n",
    "# Print the output tensor\n",
    "print(input_tensor.shape)\n",
    "print(output_tensor.shape)\n",
    "print(reward_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 152, 3584])\n",
      "torch.Size([152, 2])\n",
      "[[0.9999293088912964]]\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\n",
    "    conversation_str, \n",
    "    return_tensors=\"pt\", \n",
    ").to(model6.device)\n",
    "\n",
    "outputs = model6(input_ids=input_ids)\n",
    "print(outputs[0].shape)\n",
    "outputs = reward_model.score(outputs[0].to(reward_model.device))\n",
    "print(outputs[0].shape)\n",
    "step_sep_id = tokenizer.encode(\"<extra_0>\")[0]\n",
    "token_masks = (input_ids == step_sep_id)\n",
    "\n",
    "logits = outputs[0].cpu()\n",
    "probabilities = F.softmax(logits, dim=-1)\n",
    "probabilities = probabilities * token_masks.unsqueeze(-1) # bs, seq_len, num_labels\n",
    "\n",
    "all_scores_res = []\n",
    "for i in range(probabilities.size(0)):\n",
    "    sample = probabilities[i] # seq_len, num_labels\n",
    "    positive_probs = sample[sample != 0].view(-1, 2)[:, 1] # valid_tokens, num_labels\n",
    "    non_zero_elements_list = positive_probs.cpu().tolist()\n",
    "    all_scores_res.append(non_zero_elements_list)   \n",
    "step_reward = all_scores_res\n",
    "print(step_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.03 GiB. GPU 0 has a total capacity of 79.14 GiB of which 52.75 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.00 GiB is allocated by PyTorch, and 596.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[158], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m base_outputs \u001b[38;5;241m=\u001b[39m model6(input_ids\u001b[38;5;241m=\u001b[39minput_ids)\n\u001b[0;32m----> 2\u001b[0m lm_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m lm_outputs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(lm_outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(lm_outputs\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/spec/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/spec/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/spec/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 165\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hf_hook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/miniconda3/envs/spec/lib/python3.10/site-packages/accelerate/hooks.py:355\u001b[0m, in \u001b[0;36mAlignDevicesHook.pre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    348\u001b[0m             value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    349\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    350\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdata_ptr() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map\n\u001b[1;32m    351\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map[value\u001b[38;5;241m.\u001b[39mdata_ptr()]\n\u001b[1;32m    352\u001b[0m         ):\n\u001b[1;32m    353\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_pointers_to_remove\u001b[38;5;241m.\u001b[39madd((value\u001b[38;5;241m.\u001b[39mdata_ptr(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device))\n\u001b[0;32m--> 355\u001b[0m         \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfp16_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtied_params_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device), send_to_device(\n\u001b[1;32m    365\u001b[0m     kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device, skip_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_keys\n\u001b[1;32m    366\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/spec/lib/python3.10/site-packages/accelerate/utils/modeling.py:330\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    328\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 330\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.03 GiB. GPU 0 has a total capacity of 79.14 GiB of which 52.75 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.00 GiB is allocated by PyTorch, and 596.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "base_outputs = model6(input_ids=input_ids)\n",
    "lm_outputs = model.lm_head(base_outputs[0].to(model.device))\n",
    "lm_outputs = F.softmax(lm_outputs, dim=-1)\n",
    "print(lm_outputs.shape)\n",
    "next_token = torch.argmax(lm_outputs[0,-1]).item()\n",
    "print(next_token)\n",
    "tokenizer6.decode(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_outputs = np.exp(lm_outputs.cpu().detach().numpy())/np.exp(lm_outputs.cpu().detach().numpy()).sum(axis=-1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 152, 152064)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1283485\n",
      "?\n",
      " ?\n",
      "0.9992009\n",
      "os os\n",
      "0.96436816\n",
      " flaming  flaming\n",
      "0.1250466\n",
      " flame  plastic\n",
      "0.26603016\n",
      " white  white\n",
      "0.0038795138\n",
      " front  than\n",
      "0.012317387\n",
      " there  out\n",
      "0.020536324\n",
      " than  were\n",
      "0.99821174\n",
      "os os\n",
      "0.94122154\n",
      " flaming  flaming\n",
      "0.00017326183\n",
      " flaming  plastic\n",
      "0.9572978\n",
      " pink  pink\n",
      "1.3871845e-08\n",
      " flaming  more\n",
      "0.7323292\n",
      " many  many\n",
      "0.000980201\n",
      " Sue  how\n",
      "Cumulative log probability: -53.42704764200607\n"
     ]
    }
   ],
   "source": [
    "input_ids[0]\n",
    "log_probs = []\n",
    "beginning_offset = 9\n",
    "total_tokens = 15\n",
    "for offset in range(beginning_offset, beginning_offset+total_tokens):\n",
    "    prob = lm_outputs[0,-offset-1,input_ids[0][-offset]]\n",
    "    print(prob)\n",
    "    print(tokenizer6.decode(lm_outputs[0,-offset-1,:].argmax()),tokenizer6.decode(input_ids[0][-offset]))\n",
    "    log_probs.append(np.log(prob))\n",
    "    # prob = lm_outputs[0,-offset-2,input_ids[0][-offset]]\n",
    "    # print(prob)\n",
    "    # prob = lm_outputs[0,-offset-3,input_ids[0][-offset]]\n",
    "    # print(prob)\n",
    "    # prob = lm_outputs[0,-offset,input_ids[0][-offset]]\n",
    "    # print(prob)\n",
    "    # pred_token = lm_outputs[0,-offset,:].argmax()\n",
    "    # print(tokenizer6.decode(pred_token))\n",
    "print(f\"Cumulative log probability: {sum(log_probs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([151644,   8948,    198,   5501,   2874,   3019,    553,   3019,     11,\n",
       "           323,   2182,    697,   1590,   4226,   2878,   1124,  79075,  46391,\n",
       "        151645,    198, 151644,    872,    198,     50,    361,   6305,    304,\n",
       "           264,   2464,  12534,     13,    220,   3776,   9001,     11,    279,\n",
       "         18709,   6635,    311,   1486,    264,  75273,    389,  47649,     13,\n",
       "           220,   1913,   6602,   6556,     11,    279,  18709,   9099,    220,\n",
       "            16,     23,  18217,  12188,  84623,    436,    700,    389,  47649,\n",
       "           594,   4065,  19515,     13,    220,   1913,   7728,   6556,     11,\n",
       "           279,  18709,   3867,   1182,    825,   4843,    315,    279,  84623,\n",
       "           436,     11,  23983,   1105,   4158,     11,    323,   2182,   1493,\n",
       "         13631,  23983,   4158,  84623,    436,   1182,    700,    389,  47649,\n",
       "           594,   4065,  19515,     13,    220,   5005,     11,    389,   7270,\n",
       "          6556,     11,    807,   3694,   2441,    220,     16,     23,  18217,\n",
       "         12188,  84623,    436,    311,    279,   4426,     13,   2411,  37145,\n",
       "           389,   7270,     11,   1246,   1657,    803,  18217,  12188,  84623,\n",
       "           436,   1033,    700,   1091,   4158,  12188,  84623,    436,     30,\n",
       "        151645,    198, 151644,  77091,    198, 151651, 151645, 151643])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|im_start|>system\\nPlease reason step by step, and put your final answer within \\\\boxed{}.<|im_end|>\\n<|im_start|>user\\nSue lives in a fun neighborhood.  One weekend, the neighbors decided to play a prank on Sue.  On Friday morning, the neighbors placed 18 pink plastic flamingos out on Sue's front yard.  On Saturday morning, the neighbors took back one third of the flamingos, painted them white, and put these newly painted white flamingos back out on Sue's front yard.  Then, on Sunday morning, they added another 18 pink plastic flamingos to the collection. At noon on Sunday, how many more pink plastic flamingos were out than white plastic flamingos?<|im_end|>\\n<|im_start|>assistant\\n<extra_0><|im_end|><|endoftext|>\""
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer6.decode(input_ids[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergedModel(torch.nn.Module):\n",
    "    def __init__(self, base_model, reward_model, causal_model, beginning_offset, total_tokens):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.reward_model = reward_model\n",
    "        self.causal_model = causal_model\n",
    "        self.lm_head = causal_model.lm_head\n",
    "        self.reward_score = reward_model.score\n",
    "        self.beginning_offset = beginning_offset\n",
    "        self.total_tokens = total_tokens\n",
    "\n",
    "    def get_log_probs(self, lm_outputs, input_ids):\n",
    "        log_probs = []\n",
    "        for offset in range(self.beginning_offset, self.beginning_offset+self.total_tokens):\n",
    "            prob = lm_outputs[0,-offset-1,input_ids[0][-offset]]\n",
    "            log_probs.append(np.log(prob))\n",
    "        return log_probs\n",
    "    \n",
    "    def get_reward_scores(self, base_outputs):\n",
    "        reward_outputs = self.reward_score(base_outputs[0].to(self.device))\n",
    "        step_sep_id = tokenizer.encode(\"<extra_0>\")[0]\n",
    "        token_masks = (input_ids == step_sep_id)\n",
    "        logits = reward_outputs[0].cpu()\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        probabilities = probabilities * token_masks.unsqueeze(-1) # bs, seq_len, num_labels\n",
    "\n",
    "        all_scores_res = []\n",
    "        for i in range(probabilities.size(0)):\n",
    "            sample = probabilities[i] # seq_len, num_labels\n",
    "            positive_probs = sample[sample != 0].view(-1, 2)[:, 1] # valid_tokens, num_labels\n",
    "            non_zero_elements_list = positive_probs.cpu().tolist()\n",
    "            all_scores_res.append(non_zero_elements_list)   \n",
    "        return all_scores_res\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        base_outputs = self.base_model(input_ids)\n",
    "        lm_outputs = self.lm_head(base_outputs[0].to(self.device))\n",
    "        reward_outputs = self.reward_score(base_outputs[0].to(self.device))\n",
    "        return lm_outputs, reward_outputs\n",
    "\n",
    "    def run_merged_model(self, input_ids):\n",
    "        base_outputs = self.base_model(input_ids)\n",
    "        lm_outputs = self.lm_head(base_outputs[0].to(self.device))\n",
    "        rewards = self.get_reward_scores(base_outputs)\n",
    "        log_probs = self.get_log_probs(lm_outputs, input_ids)\n",
    "        return rewards, log_probs\n",
    "    \n",
    "    \n",
    "\n",
    "# outputs = model6(input_ids=input_ids)\n",
    "# print(outputs[0].shape)\n",
    "# outputs = reward_model.score(outputs[0].to(reward_model.device))\n",
    "# print(outputs[0].shape)\n",
    "# step_sep_id = tokenizer.encode(\"<extra_0>\")[0]\n",
    "# token_masks = (input_ids == step_sep_id)\n",
    "\n",
    "# logits = outputs[0].cpu()\n",
    "# probabilities = F.softmax(logits, dim=-1)\n",
    "# probabilities = probabilities * token_masks.unsqueeze(-1) # bs, seq_len, num_labels\n",
    "\n",
    "# all_scores_res = []\n",
    "# for i in range(probabilities.size(0)):\n",
    "#     sample = probabilities[i] # seq_len, num_labels\n",
    "#     positive_probs = sample[sample != 0].view(-1, 2)[:, 1] # valid_tokens, num_labels\n",
    "#     non_zero_elements_list = positive_probs.cpu().tolist()\n",
    "#     all_scores_res.append(non_zero_elements_list)   \n",
    "# step_reward = all_scores_res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
