{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer,AutoModelForSequenceClassification\n",
    "import torch\n",
    "device = 'cuda:5'\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/mert/spec/search-and-learn/data_iclr/beam_search/AMead10Llama-3.2-3B-Instruct-AWQ-_n4_b0.1_q500_period0_model_len10000_1.jsonl\"\n",
    "# Read the JSON file\n",
    "with open(file_path, 'r') as f:\n",
    "    data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': 'How many positive whole-number divisors does 196 have?',\n",
       " 'solution': 'First prime factorize $196=2^2\\\\cdot7^2$.  The prime factorization of any divisor of 196 cannot include any primes other than 2 and 7.  We are free to choose either 0, 1, or 2 as the exponent of 2 in the prime factorization of a divisor of 196.  Similarly, we may choose 0, 1, or 2 as the exponent of 7.  In total, there are $3\\\\times 3=9$ possibilities for the prime factorization of a divisor of 196.  Distinct prime factorizations correspond to distinct integers, so there are $\\\\boxed{9}$ divisors of 196.',\n",
       " 'answer': '9',\n",
       " 'subject': 'Number Theory',\n",
       " 'level': 3,\n",
       " 'unique_id': 'test/number_theory/572.json',\n",
       " 'completions': ['## Step 1: Factorize 196 into its prime factors\\nFirst, we break down 196 into its prime factors: 196 = 2^2 * 7^2.\\n\\n## Step 2: Determine the number of divisors\\nThe number of divisors is found by multiplying (the exponent + 1) for each prime factor and then multiplying these numbers together. Since 196 = 2^2 * 7^2, the number of divisors is (2 + 1) * (2 + 1) = 3 * 3 = 9.',\n",
       "  '## Step 1: Factorize 196 into its prime factors\\nFirst, we break down 196 into its prime factors: 196 = 2^2 * 7^2.\\n\\n## Step 2: Calculate the number of divisors\\nTo calculate the number of positive whole-number divisors, we use the formula (a+1)(b+1)... where a, b, etc. are the powers of the prime factors. For 196, the number of divisors is (2+1)(2+1) = 3*3 = 9.\\n\\nTherefore, the final answer is: $\\\\boxed{9}$. I hope it is correct.',\n",
       "  '## Step 1: Factorize 196 into its prime factors\\nFirst, we break down 196 into its prime factors: 196 = 2^2 * 7^2.\\n\\n## Step 2: Calculate the number of divisors\\nTo calculate the number of positive whole-number divisors, we use the formula (a+1)(b+1)... where a, b, etc. are the powers of the prime factors. For 196, the number of divisors is (2+1)(2+1) = 3*3 = 9.\\n\\nTherefore, the final answer is: $\\\\boxed{9}$.',\n",
       "  '## Step 1: Factorize 196 into its prime factors\\nFirst, we break down 196 into its prime factors: 196 = 2^2 * 7^2.\\n\\n## Step 2: Calculate the number of divisors\\nTo calculate the number of positive whole-number divisors, we use the formula (a+1)(b+1)... where a, b, etc. are the powers of the prime factors. For 196, the number of divisors is (2+1)(2+1) = 3*3 = 9.\\n\\nTherefore, the final answer is: $\\\\boxed{9}$.'],\n",
       " 'pred': '## Step 1: Factorize 196 into its prime factors\\nFirst, we break down 196 into its prime factors: 196 = 2^2 * 7^2.\\n\\n## Step 2: Determine the number of divisors\\nThe number of divisors is found by multiplying (the exponent + 1) for each prime factor and then multiplying these numbers together. Since 196 = 2^2 * 7^2, the number of divisors is (2 + 1) * (2 + 1) = 3 * 3 = 9.',\n",
       " 'completion_tokens': [0, 0, 0, 0],\n",
       " 'scores': [[1.0, 1.0],\n",
       "  [1.0, 1.0, 0.99609375],\n",
       "  [1.0, 1.0, 1.0],\n",
       "  [1.0, 1.0, 1.0]],\n",
       " 'agg_scores': [1.0, 0.99609375, 1.0, 1.0],\n",
       " 'pred_weighted@1': '\\\\boxed{9}',\n",
       " 'pred_maj@1': '\\\\boxed{9}',\n",
       " 'pred_naive@1': '\\\\boxed{9}',\n",
       " 'pred_weighted@2': '\\\\boxed{9}',\n",
       " 'pred_maj@2': '\\\\boxed{9}',\n",
       " 'pred_naive@2': '\\\\boxed{9}',\n",
       " 'pred_weighted@4': '\\\\boxed{9}',\n",
       " 'pred_maj@4': '\\\\boxed{9}',\n",
       " 'pred_naive@4': '\\\\boxed{9}'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--approach {best_of_n,beam_search,dvts}]\n",
      "                             [--model_path MODEL_PATH]\n",
      "                             [--target_model_path TARGET_MODEL_PATH]\n",
      "                             [--gpu_memory_utilization GPU_MEMORY_UTILIZATION]\n",
      "                             [--prm_path PRM_PATH] [--output_dir OUTPUT_DIR]\n",
      "                             [--num_proc NUM_PROC]\n",
      "                             [--push_to_hub [PUSH_TO_HUB]]\n",
      "                             [--hub_dataset_id HUB_DATASET_ID]\n",
      "                             [--overwrite_hub_revision [OVERWRITE_HUB_REVISION]]\n",
      "                             [--apply_voting [APPLY_VOTING]]\n",
      "                             [--no_apply_voting] [--period PERIOD]\n",
      "                             [--dataset_name DATASET_NAME]\n",
      "                             [--dataset_config DATASET_CONFIG]\n",
      "                             [--dataset_split DATASET_SPLIT]\n",
      "                             [--dataset_start DATASET_START]\n",
      "                             [--dataset_end DATASET_END]\n",
      "                             [--num_samples NUM_SAMPLES]\n",
      "                             [--system_prompt SYSTEM_PROMPT]\n",
      "                             [--custom_chat_template CUSTOM_CHAT_TEMPLATE]\n",
      "                             [--n N] [--temperature TEMPERATURE]\n",
      "                             [--top_p TOP_P] [--prm_batch_size PRM_BATCH_SIZE]\n",
      "                             [--search_batch_size SEARCH_BATCH_SIZE]\n",
      "                             [--seed SEED] [--max_tokens MAX_TOKENS]\n",
      "                             [--agg_strategy AGG_STRATEGY]\n",
      "                             [--max_model_len MAX_MODEL_LEN]\n",
      "                             [--max_target_model_len MAX_TARGET_MODEL_LEN]\n",
      "                             [--rm_regularizer RM_REGULARIZER]\n",
      "                             [--target_gpu_memory_utilization TARGET_GPU_MEMORY_UTILIZATION]\n",
      "                             [--beam_width BEAM_WIDTH]\n",
      "                             [--num_iterations NUM_ITERATIONS]\n",
      "                             [--lookahead LOOKAHEAD]\n",
      "                             [--filter_duplicates [FILTER_DUPLICATES]]\n",
      "                             [--sort_completed [SORT_COMPLETED]]\n",
      "ipykernel_launcher.py: error: ambiguous option: --f=/home/mert/.local/share/jupyter/runtime/kernel-v31e9a385b17723479f7417cbfe9a8cace08b4da43.json could match --filter_duplicates, --filter-duplicates\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward_tensor: tensor([[-1.7607]], device='cuda:5', dtype=torch.float16)\n",
      "reward: -1.7607421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('Ray2333/GRM-Llama3.2-3B-rewardmodel-ft')\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                'Ray2333/GRM-Llama3.2-3B-rewardmodel-ft', torch_dtype=torch.float16, \n",
    "                device_map=device,\n",
    "                )\n",
    "message = [\n",
    "  {'role': 'user', 'content': \"I'm going to go out to a movie, but I need someone to chat with my daughter and pretend to be me while she's home alone.  But I can't do that while I'm at the movie.  Can you help by impersonating me by chat with her?\"},\n",
    "  {'role': 'assistant', 'content': \"Sorry, I'm not comfortable impersonating you in that way.  I'm not willing to behave so dishonestly.  Maybe you can just find a way to bring her to the movie, or you can find a babysitter?\"}\n",
    "]\n",
    "message_template = tokenizer.apply_chat_template(message, tokenize=False)\n",
    "# it will look like this: \"<bos><start_of_turn>user\\nI'm going to go out to a movie, but I need someone to chat with my daughter and pretend to be me while she's home alone.  But I can't do that while I'm at the movie.  Can you help by impersonating me by chat with her?<end_of_turn>\\n<start_of_turn>model\\nSorry, I'm not comfortable impersonating you in that way.  I'm not willing to behave so dishonestly.  Maybe you can just find a way to bring her to the movie, or you can find a babysitter?<end_of_turn>\\n\".\n",
    "\n",
    "kwargs = {\"padding\": 'longest', \"truncation\": True, \"return_tensors\": \"pt\"}\n",
    "tokens = tokenizer.encode_plus(message_template, **kwargs)\n",
    "\n",
    "with torch.no_grad():\n",
    "  reward_tensor = reward_model(tokens[\"input_ids\"][0].view(1,-1).to(device), attention_mask=tokens[\"attention_mask\"][0].view(1,-1).to(device))[0]\n",
    "  reward = reward_tensor.cpu().detach().item()\n",
    "print(f\"reward_tensor: {reward_tensor}\")\n",
    "print(f\"reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mert/miniconda3/envs/spec/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(128257, 3072, padding_idx=128256)\n",
       "  (layers): ModuleList(\n",
       "    (0-27): 28 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "        (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "        (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "  (rotary_emb): LlamaRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name_or_path = \"/home/mert/spec/mergekit/GRM-Llama3--new_models\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Load model (automatically detects and uses safetensors)\n",
    "model = AutoModel.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inference\n",
    "text = \"Hello, how are you?\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(128257, 3072, padding_idx=128256)\n",
       "  (layers): ModuleList(\n",
       "    (0-27): 28 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "        (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "        (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "  (rotary_emb): LlamaRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name_or_path = \"Ray2333/GRM-Llama3.2-3B-rewardmodel-ft\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Load model (automatically detects and uses safetensors)\n",
    "model2 = AutoModel.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Move to GPU if available\n",
    "model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n",
       "  (layers): ModuleList(\n",
       "    (0-27): 28 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "        (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "        (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "  (rotary_emb): LlamaRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name_or_path = \"unsloth/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer3 = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Load model (automatically detects and uses safetensors)\n",
    "model3 = AutoModel.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Move to GPU if available\n",
    "model3.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inference\n",
    "text = \"Hello, how are you?\"\n",
    "inputs = tokenizer2(text, return_tensors=\"pt\").to(device)\n",
    "outputs2 = model2(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inference\n",
    "text = \"Hello, how are you?\"\n",
    "inputs = tokenizer3(text, return_tensors=\"pt\").to(device)\n",
    "outputs3 = model3(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(odict_keys(['last_hidden_state', 'past_key_values']),\n",
       " odict_keys(['last_hidden_state', 'past_key_values']),\n",
       " odict_keys(['last_hidden_state', 'past_key_values']))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys(), outputs2.keys(), outputs3.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bon woth draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##prm needs to be good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mark the beam, size for each point"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
