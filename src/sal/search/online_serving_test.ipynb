{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "openai_api_key = \"EMPTY\"\n",
    "\n",
    "args = {\n",
    "    \"draft_model_path_rsd\": \"http://localhost:1234/v1\",\n",
    "    \"target_model_path_rsd\": \"http://localhost:1234/v1\",\n",
    "    \"prm_ip_address_rsd\": \"http://localhost:1234/v1\",\n",
    "    \"prm_path_rsd\": \"http://localhost:1234/v1\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draft_client = OpenAI(\n",
    "        api_key=openai_api_key,\n",
    "        base_url=args.draft_model_path_rsd,\n",
    "    )\n",
    "draft_tokenizer = AutoTokenizer.from_pretrained(args.draft_model_path_rsd, trust_remote_code=True)\n",
    "\n",
    "target_client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=args.target_model_path_rsd,\n",
    ")\n",
    "target_tokenizer = AutoTokenizer.from_pretrained(args.target_model_path_rsd, trust_remote_code=True)\n",
    "\n",
    "prm_client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=args.prm_ip_address_rsd,\n",
    ")\n",
    "prm_tokenizer = AutoTokenizer.from_pretrained(args.prm_path_rsd, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"draft_model_name_or_path\": \"gpt-4o-mini\",\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_p\": 0.9,\n",
    "    \"max_tokens\": 2048\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_prompts = [\n",
    "            \"Hello\",\n",
    "            \"Selam\"\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = draft_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draft_responses = llm.completions.create(\n",
    "                model=config.draft_model_name_or_path.split(\"/\")[-1],\n",
    "                prompt=gen_prompts,\n",
    "                temperature=config.temperature,\n",
    "                top_p=config.top_p,\n",
    "                max_tokens=config.max_tokens,\n",
    "                stop=[\"\\n\\n\"],\n",
    "                n=1,\n",
    "                stream=False,\n",
    "                extra_body_params={\n",
    "                    \"include_stop_str_in_output\": True\n",
    "                                   }\n",
    "            ).choices\n",
    "llm_outputs = sorted(draft_responses, key=lambda x: int(x.index))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
