{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRAFT_MODEL=\"Qwen/Qwen2.5-Math-1.5B-Instruct\"\n",
    "TARGET_MODEL=\"Qwen/Qwen2.5-Math-7B-Instruct\"\n",
    "PRM=\"Skywork/Skywork-o1-Open-PRM-Qwen-2.5-1.5B\"\n",
    "DRAFT_IP_ADDRESS=\"http://localhost:12340/v1\"\n",
    "TARGET_IP_ADDRESS=\"http://localhost:12341/v1\"\n",
    "PRM_IP_ADDRESS=\"http://localhost:12342/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mert_cemri/miniconda3/envs/spec/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "openai_api_key = \"EMPTY\"\n",
    "\n",
    "args = {\n",
    "    \"draft_model_path_rsd\": DRAFT_MODEL,\n",
    "    \"draft_model_ip_address\": DRAFT_IP_ADDRESS,\n",
    "    \"target_model_path_rsd\": TARGET_MODEL,\n",
    "    \"target_model_ip_address\": TARGET_IP_ADDRESS,\n",
    "    \"prm_ip_address_rsd\": PRM_IP_ADDRESS,\n",
    "    \"prm_path_rsd\": PRM\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draft_client = OpenAI(\n",
    "#         api_key=openai_api_key,\n",
    "#         base_url=args[\"draft_model_ip_address\"],\n",
    "#     )\n",
    "# draft_tokenizer = AutoTokenizer.from_pretrained(args[\"draft_model_path_rsd\"], trust_remote_code=True)\n",
    "\n",
    "target_client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=args[\"target_model_ip_address\"],\n",
    ")\n",
    "target_tokenizer = AutoTokenizer.from_pretrained(args[\"target_model_path_rsd\"], trust_remote_code=True)\n",
    "\n",
    "prm_client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=args[\"prm_ip_address_rsd\"],\n",
    ")\n",
    "prm_tokenizer = AutoTokenizer.from_pretrained(args[\"prm_path_rsd\"], trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if we can get the logits from the target model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Hello, how are you?\"\n",
    "input_ids = target_tokenizer.encode(input_text)\n",
    "\n",
    "prm_model_logits = prm_client.embeddings.create(\n",
    "                model=args[\"prm_path_rsd\"].split(\"/\")[-1],\n",
    "                input=input_ids,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.283203125, -0.271484375, -1.0, -1.296875, -2.328125, 0.291015625]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prm_model_logits.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_text = \"Hello, how are you?\"\n",
    "# input_ids = target_tokenizer.encode(input_text)\n",
    "\n",
    "# target_model_logits = target_client.embeddings.create(\n",
    "#                 model=args[\"target_model_path_rsd\"].split(\"/\")[-1],\n",
    "#                 input=input_ids,\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 01:50:09,871\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-28 01:50:10 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 02-28 01:50:10 __init__.py:30] Available plugins for group vllm.general_plugins:\n",
      "INFO 02-28 01:50:10 __init__.py:32] name=register_dummy_model, value=vllm_add_dummy_model.prm_model:register\n",
      "INFO 02-28 01:50:10 __init__.py:34] all available plugins for group vllm.general_plugins will be loaded.\n",
      "INFO 02-28 01:50:10 __init__.py:36] set environment variable VLLM_PLUGINS to control which plugins to load.\n",
      "ERROR 02-28 01:50:10 __init__.py:46] Failed to load plugin register_dummy_model\n",
      "ERROR 02-28 01:50:10 __init__.py:46] Traceback (most recent call last):\n",
      "ERROR 02-28 01:50:10 __init__.py:46]   File \"/data/mert_cemri/miniconda3/envs/spec/lib/python3.11/site-packages/vllm/plugins/__init__.py\", line 42, in load_plugins_by_group\n",
      "ERROR 02-28 01:50:10 __init__.py:46]     func = plugin.load()\n",
      "ERROR 02-28 01:50:10 __init__.py:46]            ^^^^^^^^^^^^^\n",
      "ERROR 02-28 01:50:10 __init__.py:46]   File \"/data/mert_cemri/miniconda3/envs/spec/lib/python3.11/importlib/metadata/__init__.py\", line 198, in load\n",
      "ERROR 02-28 01:50:10 __init__.py:46]     module = import_module(match.group('module'))\n",
      "ERROR 02-28 01:50:10 __init__.py:46]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 02-28 01:50:10 __init__.py:46]   File \"/data/mert_cemri/miniconda3/envs/spec/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "ERROR 02-28 01:50:10 __init__.py:46]     return _bootstrap._gcd_import(name[level:], package, level)\n",
      "ERROR 02-28 01:50:10 __init__.py:46]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 02-28 01:50:10 __init__.py:46]   File \"<frozen importlib._bootstrap>\", line 1206, in _gcd_import\n",
      "ERROR 02-28 01:50:10 __init__.py:46]   File \"<frozen importlib._bootstrap>\", line 1178, in _find_and_load\n",
      "ERROR 02-28 01:50:10 __init__.py:46]   File \"<frozen importlib._bootstrap>\", line 1149, in _find_and_load_unlocked\n",
      "ERROR 02-28 01:50:10 __init__.py:46]   File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
      "ERROR 02-28 01:50:10 __init__.py:46]   File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
      "ERROR 02-28 01:50:10 __init__.py:46]   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "ERROR 02-28 01:50:10 __init__.py:46]   File \"/data/mert_cemri/spec/RSD/skywork-o1-prm-inference/vllm_add_dummy_model/prm_model.py\", line 7, in <module>\n",
      "ERROR 02-28 01:50:10 __init__.py:46]     from vllm.model_executor.layers.pooler import (\n",
      "ERROR 02-28 01:50:10 __init__.py:46] ImportError: cannot import name 'EmbeddingSequenceGroupOutput' from 'vllm.model_executor.layers.pooler' (/data/mert_cemri/miniconda3/envs/spec/lib/python3.11/site-packages/vllm/model_executor/layers/pooler.py)\n",
      "INFO 02-28 01:50:15 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='Qwen/Qwen2.5-Math-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-Math-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=PoolerConfig(pooling_type=None, normalize=None, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 02-28 01:50:17 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 02-28 01:50:17 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-Math-7B-Instruct...\n",
      "INFO 02-28 01:50:18 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.86it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.73it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.02it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.88it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.88it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-28 01:50:20 model_runner.py:1115] Loading model weights took 14.2419 GB\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "# Load the target model\n",
    "target_model = LLM(\n",
    "    model=\"Qwen/Qwen2.5-Math-7B-Instruct\",\n",
    "    tensor_parallel_size=1,\n",
    "    task=\"embed\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.99it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n"
     ]
    }
   ],
   "source": [
    "encodings = target_model.embed(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[EmbeddingRequestOutput(request_id='0', outputs=EmbeddingOutput(hidden_size=3584), prompt_token_ids=[3838, 374, 279, 6722, 315, 9625, 30], finished=True)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingOutput(hidden_size=3584)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings[0].outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"draft_model_name_or_path\": DRAFT_MODEL,\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_p\": 0.9,\n",
    "    \"max_tokens\": 2048\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_prompts = [\n",
    "            \"Say hello\",\n",
    "            \"Say goodbye\"\n",
    "        ]\n",
    "llm = draft_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "draft_responses = llm.completions.create(\n",
    "                model=config[\"draft_model_name_or_path\"].split(\"/\")[-1],\n",
    "                prompt=gen_prompts,\n",
    "                temperature=config[\"temperature\"],\n",
    "                top_p=config[\"top_p\"],\n",
    "                max_tokens=config[\"max_tokens\"],\n",
    "                stop=[\"\\n\\n\"],\n",
    "                n=1,\n",
    "                stream=False,\n",
    "                extra_body={\n",
    "                    \"include_stop_str_in_output\": True\n",
    "                                   }\n",
    "            ).choices\n",
    "llm_outputs = sorted(draft_responses, key=lambda x: int(x.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"llm_outputs: \\n\", llm_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output in llm_outputs:\n",
    "    print(f\"output.text: {output.text}\")\n",
    "    print(f\"output.index: {output.index}\")\n",
    "    print(f\"output.finish_reason: {output.finish_reason}\")\n",
    "    print(f\"output.logprobs: {output.logprobs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(problem, response, tokenizer, step_token):\n",
    "    prompt_ids = tokenizer.encode(tokenizer.bos_token + problem + \"\\n\")\n",
    "    response_ids = []\n",
    "    steps = []\n",
    "    reward_flags = [0] * len(prompt_ids)\n",
    "    step_token_id = tokenizer.encode(step_token)[-1]\n",
    "    for idx, step in enumerate(response.split(step_token)):\n",
    "        if step != \"\":\n",
    "            step_ids = tokenizer.encode(step)\n",
    "        else:\n",
    "            step_ids = []\n",
    "        step_ids += [step_token_id]\n",
    "        step = step + step_token\n",
    "        flag = [0] * len(step_ids)\n",
    "        flag[-1] = 1\n",
    "        response_ids.extend(step_ids)\n",
    "        reward_flags.extend(flag)\n",
    "        steps.append(step)\n",
    "    input_ids = prompt_ids + response_ids\n",
    "    return input_ids, steps, reward_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = [\n",
    "    prepare_input(p, full_resp.text, tokenizer=prm_tokenizer, step_token=\"\\n\\n\") \n",
    "    for p, full_resp in zip(gen_prompts, llm_outputs)\n",
    "]\n",
    "input_ids, steps, reward_flags = zip(*processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = prm_client.embeddings.create(\n",
    "                model=args[\"prm_path_rsd\"].split(\"/\")[-1],\n",
    "                input=input_ids,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in rewards:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(np.exp(-x) + 1)\n",
    "    \n",
    "def derive_step_rewards_vllm(raw_rewards, batch_reward_flags):\n",
    "    batch_step_rewards = []\n",
    "    for idx,data in enumerate(raw_rewards.data):\n",
    "        rewards = data.embedding\n",
    "        reward_flags = batch_reward_flags[idx]\n",
    "\n",
    "        step_rewards = [sigmoid(reward) for reward,flag in zip(rewards,reward_flags) if flag == 1]   \n",
    "        batch_step_rewards.append(step_rewards)\n",
    "    return batch_step_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_rewards = derive_step_rewards_vllm(rewards, reward_flags)\n",
    "step_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model_path = \"/data/user_data/mert/spec/models_merged/Qwen2--qwen_7b_merged\"\n",
    "reward_model_name = \"Qwen/Qwen2.5-Math-PRM-7B\"\n",
    "causal_model_name = \"Qwen/Qwen2.5-7B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
