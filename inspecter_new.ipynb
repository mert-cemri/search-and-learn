{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer,AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:5'\n",
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('Ray2333/GRM-Llama3.2-3B-rewardmodel-ft')\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                'Ray2333/GRM-Llama3.2-3B-rewardmodel-ft', torch_dtype=torch.float16, \n",
    "                device_map=device,\n",
    "                )\n",
    "message = [\n",
    "  {'role': 'user', 'content': \"I'm going to go out to a movie, but I need someone to chat with my daughter and pretend to be me while she's home alone.  But I can't do that while I'm at the movie.  Can you help by impersonating me by chat with her?\"},\n",
    "  {'role': 'assistant', 'content': \"Sorry, I'm not comfortable impersonating you in that way.  I'm not willing to behave so dishonestly.  Maybe you can just find a way to bring her to the movie, or you can find a babysitter?\"}\n",
    "]\n",
    "message_template = tokenizer.apply_chat_template(message, tokenize=False)\n",
    "# it will look like this: \"<bos><start_of_turn>user\\nI'm going to go out to a movie, but I need someone to chat with my daughter and pretend to be me while she's home alone.  But I can't do that while I'm at the movie.  Can you help by impersonating me by chat with her?<end_of_turn>\\n<start_of_turn>model\\nSorry, I'm not comfortable impersonating you in that way.  I'm not willing to behave so dishonestly.  Maybe you can just find a way to bring her to the movie, or you can find a babysitter?<end_of_turn>\\n\".\n",
    "\n",
    "kwargs = {\"padding\": 'longest', \"truncation\": True, \"return_tensors\": \"pt\"}\n",
    "tokens = tokenizer.encode_plus(message_template, **kwargs)\n",
    "\n",
    "with torch.no_grad():\n",
    "  reward_tensor = reward_model(tokens[\"input_ids\"][0].view(1,-1).to(device), attention_mask=tokens[\"attention_mask\"][0].view(1,-1).to(device))[0]\n",
    "  reward = reward_tensor.cpu().detach().item()\n",
    "print(f\"reward_tensor: {reward_tensor}\")\n",
    "print(f\"reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"/home/mert/spec/mergekit/GRM-Llama3--new_models\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Load model (automatically detects and uses safetensors)\n",
    "model = AutoModel.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inference\n",
    "text = \"Hello, how are you?\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"Ray2333/GRM-Llama3.2-3B-rewardmodel-ft\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Load model (automatically detects and uses safetensors)\n",
    "model2 = AutoModel.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Move to GPU if available\n",
    "model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"unsloth/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer3 = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Load model (automatically detects and uses safetensors)\n",
    "model3 = AutoModel.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Move to GPU if available\n",
    "model3.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inference\n",
    "text = \"Hello, how are you?\"\n",
    "inputs = tokenizer2(text, return_tensors=\"pt\").to(device)\n",
    "outputs2 = model2(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inference\n",
    "text = \"Hello, how are you?\"\n",
    "inputs = tokenizer3(text, return_tensors=\"pt\").to(device)\n",
    "outputs3 = model3(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.keys(), outputs2.keys(), outputs3.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bon woth draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##prm needs to be good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mark the beam, size for each point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bon with 3times with 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Union, Any\n",
    "def load_jsonl(file: Union[str, Path]) -> Iterable[Any]:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                yield json.loads(line)\n",
    "            except Exception:\n",
    "                print(\"Error in loading:\", line)\n",
    "                exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_align_100_t128_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_align_100_t128 = \"/home/mert/spec/search-and-learn/data_iclr/speculative_beam_search/AMead10Llama-3.2-3B-Instruct-AWQ-hugging-quantsMeta-Llama-3.1-8B-Instruct-AWQ-INT4_n4_b1.5_q5_period0_t128_num_iters640.jsonl\"\n",
    "spec_align_100_t128_results = list(load_jsonl(spec_align_100_t128))\n",
    "index = 3\n",
    "print(\"pred: \", spec_align_100_t128_results[index]['pred'], \"\\n answer: \", spec_align_100_t128_results[index]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## analkyze when things go wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = spec_align_100_t128_results[index]['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import torch\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "\n",
    "# Initialize LLM specifically on GPU 2 only\n",
    "llm = LLM(model=\"AMead10/Llama-3.2-3B-Instruct-AWQ\")\n",
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and get length\n",
    "tokens = tokenizer.encode(text)\n",
    "num_tokens = len(tokens)\n",
    "\n",
    "print(f\"Number of tokens in completion: {num_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
